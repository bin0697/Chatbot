{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0642828552a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2697\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2699\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[0;32m    185\u001b[0m                                         allow_soft_placement=True)\n\u001b[1;32m--> 186\u001b[1;33m             \u001b[0m_SESSION\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m     \"\"\"\n\u001b[1;32m-> 1511\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-43e73f042c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "print (imdb.load_data().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f8b227916313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# plot graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'multilayer_perceptron_graph.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "\n",
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "model.summary()\n",
    "# plot graph\n",
    "plot_model(model, to_file='multilayer_perceptron_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [ 10247  10452  10110 122254  10005  10132  10149  94194 263461  10055\n",
      "  10047 374498  11029  10057  10607  39066  10092   9999  10405  10076\n",
      "  10040      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "ids = np.load(os.path.join('I:/NLP/data_train/','idsMatrixques-2.npy'))\n",
    "#ids = np.load(f'{currentDir}idsMatrix.npy')\n",
    "print('Word indexes of the first review: ', ids[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n",
      "Index of `ngon` in wordsList:  1906\n",
      "Vector representation of `ngon` is:  [-1.365e-01 -5.560e-02  1.195e-01  4.790e-02 -8.640e-02 -2.430e-02\n",
      " -3.130e-02 -4.500e-02  4.340e-02 -1.074e-01 -9.460e-02 -5.220e-02\n",
      " -9.380e-02 -4.480e-02 -1.220e-02 -6.000e-03  3.570e-02 -5.500e-02\n",
      " -6.990e-02  6.200e-03  9.800e-03 -4.970e-02  8.430e-02  9.160e-02\n",
      "  1.402e-01 -3.240e-02  3.970e-02 -8.270e-02 -3.770e-02 -7.240e-02\n",
      "  2.710e-02  9.450e-02 -9.180e-02 -1.171e-01  6.750e-02 -8.980e-02\n",
      "  1.854e-01 -1.097e-01 -1.230e-02  2.870e-02 -1.274e-01  5.160e-02\n",
      " -3.800e-03 -6.360e-02  4.900e-03  6.400e-02  1.466e-01 -4.950e-02\n",
      "  9.050e-02 -1.720e-02  1.684e-01  1.850e-02 -6.220e-02 -6.320e-02\n",
      " -9.450e-02 -7.700e-02  1.410e-02 -5.110e-02 -2.300e-03  7.680e-02\n",
      " -5.090e-02 -9.790e-02  2.660e-02  1.153e-01  3.750e-02  1.185e-01\n",
      "  4.650e-02  6.500e-03  1.913e-01  7.500e-02  6.960e-02 -6.130e-02\n",
      " -5.210e-02 -5.870e-02  5.530e-02  4.530e-02  3.770e-02 -9.500e-03\n",
      " -5.860e-02  1.187e-01  8.800e-03 -5.590e-02 -9.490e-02 -1.969e-01\n",
      " -8.430e-02 -1.160e-02  3.590e-02  4.070e-02 -4.020e-02  8.170e-02\n",
      "  7.110e-02  3.170e-02 -1.597e-01 -5.290e-02 -7.000e-03  1.852e-01\n",
      " -1.020e-02  7.180e-02  1.429e-01  7.100e-03  8.690e-02  7.630e-02\n",
      "  7.810e-02 -1.732e-01  3.790e-02 -1.025e-01 -1.390e-02  1.253e-01\n",
      " -1.940e-02 -1.036e-01 -1.684e-01 -1.010e-02 -5.920e-02 -8.340e-02\n",
      "  3.800e-03 -2.300e-02  2.810e-02 -2.660e-02  1.052e-01 -8.460e-02\n",
      " -7.120e-02 -4.930e-02 -7.900e-03  2.490e-02  5.660e-02  5.570e-02\n",
      "  4.860e-02 -7.550e-02  1.048e-01  2.610e-02  3.650e-02 -1.352e-01\n",
      " -1.938e-01 -5.030e-02  1.770e-01 -4.720e-02  1.247e-01  9.930e-02\n",
      "  1.320e-02 -2.930e-02 -1.305e-01 -3.410e-02  5.740e-02 -9.250e-02\n",
      "  8.210e-02  6.650e-02  6.980e-02 -4.090e-02 -1.000e-04 -5.500e-02\n",
      " -7.460e-02 -7.730e-02  2.990e-02 -5.230e-02  4.180e-02  5.340e-02\n",
      "  2.560e-02 -1.060e-02  1.400e-02  3.930e-02  6.760e-02 -3.070e-02\n",
      " -5.260e-02 -1.300e-03 -1.600e-02  2.552e-01 -4.840e-02 -8.460e-02\n",
      " -1.333e-01 -1.300e-03  1.710e-02 -8.240e-02 -1.800e-03 -6.720e-02\n",
      " -1.223e-01  6.800e-03  2.680e-02  1.084e-01 -5.000e-03  1.085e-01\n",
      " -9.700e-02 -2.880e-02  2.079e-01 -1.180e-02  2.071e-01 -5.390e-02\n",
      " -3.300e-03  6.610e-02  8.220e-02 -6.190e-02  1.084e-01  3.550e-02\n",
      "  6.270e-02  2.430e-02  7.380e-02  2.826e-01  5.850e-02  7.500e-03\n",
      "  2.774e-01 -1.166e-01 -7.270e-02 -3.140e-02 -2.740e-02 -4.410e-02\n",
      "  7.490e-02 -1.020e-02 -1.530e-01  1.706e-01  2.160e-02 -1.091e-01\n",
      " -3.840e-02 -3.070e-02  5.800e-03 -2.520e-02  4.000e-02  2.170e-02\n",
      "  1.001e-01  2.810e-02 -9.630e-02 -1.270e-02  5.590e-02 -1.352e-01\n",
      "  7.750e-02  8.470e-02  3.170e-02 -3.100e-02  9.720e-02 -1.330e-01\n",
      "  4.000e-04 -2.455e-01  1.037e-01  2.300e-02 -2.200e-03 -1.330e-01\n",
      "  1.040e-02 -4.400e-02 -2.900e-03  3.800e-03 -1.030e-01 -1.687e-01\n",
      "  8.990e-02 -2.840e-02 -3.410e-02  4.570e-02 -1.460e-02  5.350e-02\n",
      " -9.700e-02  1.170e-02 -7.730e-02 -1.870e-02  1.330e-01  3.870e-02\n",
      "  1.425e-01 -3.190e-02  1.725e-01  1.480e-02 -2.320e-02  1.930e-01\n",
      "  1.257e-01 -5.900e-02  6.250e-02  6.700e-03 -1.404e-01 -2.820e-02\n",
      "  2.780e-02  3.090e-02 -7.000e-03 -2.660e-02  3.820e-02  1.320e-02\n",
      " -1.056e-01 -3.640e-02 -5.580e-02  3.890e-02 -2.080e-02 -3.670e-02\n",
      " -4.250e-02 -1.636e-01 -3.290e-02 -4.550e-02  2.970e-02  1.287e-01\n",
      "  1.393e-01 -1.080e-01 -4.070e-02  5.250e-02 -1.031e-01 -1.060e-02\n",
      "  6.090e-02  8.550e-02  3.520e-02 -1.066e-01 -1.100e-02  1.051e-01\n",
      "  6.740e-02 -3.930e-02 -9.590e-02 -1.300e-03 -1.234e-01  2.150e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "currentDir = 'I:/NLP/data_train/'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "#wordsList = np.load(f'{currentDir}wordsList.npy')\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "#wordVectors = np.load(f'{currentDir}wordVectors.npy')\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')\n",
    "ngon_idx = wordsList.index('bán')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180\n",
    "\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros((batchSize, maxSeqLength))\n",
    "    for i in range(batchSize):\n",
    "            # Pick positive samples randomly\n",
    "        num = randint(1, 14000)\n",
    "        labels.append([1, 0])\n",
    "       \n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      " Các bạn IU cho mình hỏi là sau khi làm xong thesis ở học kì 2 năm 4 thì mình còn học kì 3 rãnh. Liệu mình có thể đkmh để cải thiện điểm ko? Và có thể update điểm kịp để xét tốt nghiệp vào tháng 9 không nhỉ? _\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = open('I:/NLP/data_train/xuly.txt').readline()\n",
    "# Keeping only the neccessary columns\n",
    "data = [[]]\n",
    "with open('I:/NLP/data_train/xuly.txt', \"r\", encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data.append([line, 1])       \n",
    "print('Positive files finished')\n",
    "#data = [[open('I:/NLP/data_train/xuly.txt').readline(),1]]\n",
    "print (data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8eb2381319a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#data = data[data.sentiment != \"Neutral\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-z0-9\\s]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Positive'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "#data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['sentiment'] == 'Positive'].size)\n",
    "print(data[ data['sentiment'] == 'Negative'].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9448\n",
      "[  10181  314185  314185   10181   11505   11066  350794   10065   10157\n",
      "   10059   10194   10013  314185   10213   10213   10213 1264310   10005\n",
      "   10011   10013   10019   10038  275576   10210   10210   10030   10027\n",
      "   10744   10065  604258   10011   10011   10023   10545   10196   10400\n",
      "   10521   10637  368564   10011  811496   10023   10023   15980   13438\n",
      "   12983   18975  140453  611633   10062   10010  164768  701607   10017\n",
      "   13364   10005   12916   10028   10793   10101   10393       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#embed_dim = 128\n",
    "#lstm_out = 196\n",
    "x =  np.load(f'I:/NLP/data_train/idsMatrixques-2.npy')\n",
    "print (len(x))\n",
    "#X = open('I:/NLP/data_train/idsMatrixques.npy', \"r\", encoding='utf-8')\n",
    "print (x[2999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,10):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range (1,1831):\n",
    "    data.append(x[i])\n",
    "    target.append(1)\n",
    "\n",
    "for i in range (1831,7553):\n",
    "    data.append(x[i])\n",
    "    target.append(0)\n",
    "    \n",
    "for i in range (1001,1199):\n",
    "    x_test.append(x[i])\n",
    "    y_test.append(1)\n",
    "for i in range (2003,2267):\n",
    "    x_test.append(x[i])\n",
    "    y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10247  10452  10110 122254  10005  10132  10149  94194 263461  10055\n",
      "  10047 374498  11029  10057  10607  39066  10092   9999  10405  10076\n",
      "  10040      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (data[1])\n",
    "print (target[1831])\n",
    "# target = np.array(target)\n",
    "# target = target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_weight_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6ec849cedebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedding_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_weight_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0membedding_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# create the embedding layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_weight_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "def get_weight(ids):\n",
    "    return wordVectors[ids]   \n",
    "\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "embedding_vectors = wordVectors[idx]\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors = np.load(f'I:/NLP/data_train/wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index is: 3777\n",
      "300\n",
      "[ 2.529e-01  1.483e-01  8.210e-02 -4.320e-02 -1.104e-01  1.000e-04\n",
      " -9.490e-02 -5.620e-02  1.540e-02 -4.410e-02 -7.520e-02 -7.640e-02\n",
      " -1.490e-01  6.840e-02 -2.175e-01  9.160e-02 -5.800e-02  7.780e-02\n",
      " -6.300e-02 -7.510e-02 -1.332e-01  1.229e-01 -2.330e-02  1.745e-01\n",
      "  6.930e-02  9.900e-03 -6.200e-03 -2.029e-01  3.051e-01  1.452e-01\n",
      "  1.280e-02 -8.600e-02  8.600e-02 -2.049e-01  5.670e-02  3.400e-02\n",
      "  7.320e-02  1.021e-01  2.271e-01 -6.200e-03  6.680e-02 -4.400e-02\n",
      " -3.550e-02 -1.461e-01 -1.370e-02  9.480e-02 -8.350e-02 -1.979e-01\n",
      " -1.392e-01  7.600e-03 -1.030e-02  3.234e-01 -7.090e-02  3.067e-01\n",
      " -1.561e-01 -2.104e-01 -4.100e-03 -1.005e-01 -2.084e-01 -9.150e-02\n",
      "  1.850e-02 -8.550e-02 -6.080e-02 -5.290e-02 -2.180e-02  8.650e-02\n",
      "  4.920e-02 -1.770e-02 -2.360e-02 -7.200e-03  2.890e-02 -1.548e-01\n",
      "  1.010e-01  1.579e-01 -1.187e-01  1.853e-01  1.319e-01  1.792e-01\n",
      " -4.740e-02 -6.680e-02 -1.988e-01  1.730e-02  7.040e-02 -2.090e-01\n",
      "  2.990e-02  1.351e-01  1.995e-01 -1.092e-01 -1.600e-02  6.740e-02\n",
      "  9.930e-02 -2.463e-01 -2.677e-01  9.500e-03 -1.410e-02 -1.530e-02\n",
      " -4.900e-02  2.650e-02 -1.060e-01  1.500e-02  1.660e-01  8.530e-02\n",
      " -3.560e-02 -1.771e-01 -1.362e-01 -1.490e-02  6.380e-02 -7.810e-02\n",
      " -5.130e-02 -1.048e-01 -1.609e-01 -3.440e-02 -1.058e-01 -7.220e-02\n",
      "  1.756e-01 -3.450e-02  1.070e-02 -4.840e-02 -5.990e-02 -8.730e-02\n",
      "  4.330e-02 -7.030e-02  5.200e-02 -5.080e-02  1.185e-01 -6.180e-02\n",
      " -2.084e-01 -2.283e-01  9.040e-02  3.010e-02  1.370e-01 -8.810e-02\n",
      " -1.589e-01  1.043e-01  9.030e-02 -1.408e-01 -1.576e-01  8.670e-02\n",
      "  1.177e-01 -2.004e-01  2.521e-01  3.950e-02  7.670e-02  7.820e-02\n",
      "  5.620e-02  1.004e-01 -2.816e-01 -2.187e-01  3.980e-02 -8.000e-03\n",
      " -1.751e-01 -1.554e-01  8.040e-02 -6.840e-02  1.410e-02  1.140e-02\n",
      "  2.920e-01  5.270e-02 -1.300e-01  1.900e-02  2.770e-02  5.130e-02\n",
      " -2.900e-03 -5.500e-02 -4.720e-02  2.742e-01  1.236e-01 -8.500e-02\n",
      " -1.750e-02  2.440e-02  1.241e-01 -5.120e-02  1.405e-01 -2.826e-01\n",
      " -1.248e-01 -2.011e-01 -8.220e-02 -4.500e-03 -1.889e-01 -7.900e-03\n",
      "  7.540e-02  1.071e-01  1.619e-01  6.880e-02  1.023e-01 -7.530e-02\n",
      "  7.280e-02  5.820e-02 -1.200e-03 -2.089e-01 -1.911e-01  4.700e-03\n",
      " -1.670e-02 -8.600e-03 -4.630e-02  1.666e-01  1.619e-01 -4.480e-02\n",
      "  1.127e-01 -1.269e-01 -2.200e-03  8.380e-02  6.560e-02 -1.973e-01\n",
      " -8.020e-02 -2.434e-01 -7.550e-02  2.100e-02 -1.732e-01  9.200e-03\n",
      "  2.600e-01 -1.205e-01  8.930e-02  5.410e-02  1.090e-02  4.000e-04\n",
      " -5.380e-02 -1.177e-01  1.930e-02  2.263e-01  1.440e-02  1.356e-01\n",
      "  9.130e-02  1.307e-01  1.282e-01  1.037e-01 -1.110e-02  7.460e-02\n",
      " -6.800e-03 -7.720e-02  2.150e-02  1.210e-02  1.727e-01 -1.850e-02\n",
      "  2.763e-01  3.030e-02  3.350e-02 -4.490e-02 -2.030e-02 -6.150e-02\n",
      " -6.000e-03 -1.074e-01  2.120e-02  1.632e-01  1.780e-02 -1.532e-01\n",
      " -9.630e-02 -1.727e-01 -1.508e-01 -2.364e-01  5.760e-02  1.947e-01\n",
      "  1.885e-01  9.610e-02  1.788e-01  3.320e-02  8.540e-02  2.031e-01\n",
      " -6.420e-02 -7.280e-02  7.180e-02 -4.940e-02 -7.080e-02  1.565e-01\n",
      " -2.120e-02  3.070e-02 -1.748e-01  5.750e-02 -6.400e-02  1.380e-02\n",
      " -7.550e-02 -8.400e-03  3.900e-02 -1.490e-02 -1.062e-01  1.290e-01\n",
      " -8.250e-02 -4.780e-02 -7.600e-03  4.920e-02 -1.032e-01  1.832e-01\n",
      "  2.372e-01 -4.070e-02 -1.096e-01 -5.530e-02 -4.010e-02  2.310e-02\n",
      "  1.790e-02  9.820e-02 -5.110e-02 -2.540e-01  3.142e-01  2.660e-02\n",
      " -1.212e-01 -1.462e-01  1.054e-01  9.300e-02  4.370e-02  1.453e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fileroad = 'E:/cc.vi.300.vec' \n",
    "tmp = np.array\n",
    "\n",
    "tuw = []\n",
    "#y = 0 \n",
    "with open(fileroad, encoding=\"utf8\") as fp:\n",
    "    for x in range(2000000):\n",
    "        line = fp.readline().strip()\n",
    "        #while line:\n",
    "        z = line.split(' ', 1)[0]\n",
    "        #if x == 19970:\n",
    "        if z == 'cám':\n",
    "            print ('index is: ' + str(x))\n",
    "            print (len(line.split()[1:]))\n",
    "            tmp = np.array(line.split()[1:], dtype='float32')\n",
    "            print(tmp)\n",
    "            # print (line.split(' ', 1)[1:])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 300)          600000000 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 600,219,777\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 600,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "7552/7552 [==============================] - 66s 9ms/step - loss: 0.5560 - acc: 0.7552\n",
      "Epoch 2/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.4949 - acc: 0.7846\n",
      "Epoch 3/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.3287 - acc: 0.8942\n",
      "Epoch 4/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.3751 - acc: 0.8742\n",
      "Epoch 5/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.3197 - acc: 0.8734\n",
      "Epoch 6/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.1246 - acc: 0.9604\n",
      "Epoch 7/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.0810 - acc: 0.9747\n",
      "Epoch 8/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0723 - acc: 0.9774\n",
      "Epoch 9/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.0608 - acc: 0.9825\n",
      "Epoch 10/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0482 - acc: 0.9862\n",
      "Epoch 11/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.0434 - acc: 0.9877\n",
      "Epoch 12/60\n",
      "7552/7552 [==============================] - 62s 8ms/step - loss: 0.0369 - acc: 0.9898\n",
      "Epoch 13/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0326 - acc: 0.9919\n",
      "Epoch 14/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0335 - acc: 0.9915\n",
      "Epoch 15/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0357 - acc: 0.9913\n",
      "Epoch 16/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0302 - acc: 0.9927\n",
      "Epoch 17/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0346 - acc: 0.9901\n",
      "Epoch 18/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0298 - acc: 0.9921\n",
      "Epoch 19/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0287 - acc: 0.9926\n",
      "Epoch 20/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0276 - acc: 0.9935\n",
      "Epoch 21/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0257 - acc: 0.9931\n",
      "Epoch 22/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0242 - acc: 0.9948\n",
      "Epoch 23/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0220 - acc: 0.9939\n",
      "Epoch 24/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0244 - acc: 0.9935\n",
      "Epoch 25/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0210 - acc: 0.9944\n",
      "Epoch 26/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0192 - acc: 0.9935\n",
      "Epoch 27/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 28/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0180 - acc: 0.9947\n",
      "Epoch 29/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0199 - acc: 0.9951\n",
      "Epoch 30/60\n",
      "7552/7552 [==============================] - 63s 8ms/step - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 31/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0156 - acc: 0.9948\n",
      "Epoch 32/60\n",
      "7552/7552 [==============================] - 64s 9ms/step - loss: 0.0180 - acc: 0.9940\n",
      "Epoch 33/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0165 - acc: 0.9952\n",
      "Epoch 34/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 35/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0164 - acc: 0.9948\n",
      "Epoch 36/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0159 - acc: 0.9958\n",
      "Epoch 37/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0128 - acc: 0.9966\n",
      "Epoch 38/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0132 - acc: 0.9970\n",
      "Epoch 39/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 40/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0140 - acc: 0.9956\n",
      "Epoch 41/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0114 - acc: 0.9970\n",
      "Epoch 42/60\n",
      "7552/7552 [==============================] - 64s 8ms/step - loss: 0.0129 - acc: 0.9964\n",
      "Epoch 43/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 44/60\n",
      "7552/7552 [==============================] - 64s 9ms/step - loss: 0.0151 - acc: 0.9952\n",
      "Epoch 45/60\n",
      "7552/7552 [==============================] - 64s 9ms/step - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 46/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0113 - acc: 0.9974\n",
      "Epoch 47/60\n",
      "7552/7552 [==============================] - 64s 9ms/step - loss: 0.0114 - acc: 0.9970\n",
      "Epoch 48/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0108 - acc: 0.9970\n",
      "Epoch 49/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0092 - acc: 0.9979\n",
      "Epoch 50/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 51/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0097 - acc: 0.9974\n",
      "Epoch 52/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0101 - acc: 0.9966\n",
      "Epoch 53/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 54/60\n",
      "7552/7552 [==============================] - 66s 9ms/step - loss: 0.0103 - acc: 0.9970\n",
      "Epoch 55/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0074 - acc: 0.9980\n",
      "Epoch 56/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0107 - acc: 0.9971\n",
      "Epoch 57/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 58/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0090 - acc: 0.9964\n",
      "Epoch 59/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0082 - acc: 0.9977\n",
      "Epoch 60/60\n",
      "7552/7552 [==============================] - 65s 9ms/step - loss: 0.0110 - acc: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14084edcc88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()  \n",
    "# model.add(LSTM(100, input_shape=(1, 100),return_sequences=True))\n",
    "# model.add(Dense(100))\n",
    "# model.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n",
    "# model.fit(data, target, nb_epoch=10, batch_size=20, verbose=2,validation_data=(x_test, y_test))\n",
    "\n",
    "max_fatures = 2000\n",
    "embed_dim = 128\n",
    "lstm_out = 196 \n",
    "# model.add(Embedding(max_fatures, embed_dim,input_length = 185))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# model.fit(data, target, nb_epoch=10, batch_size=20, verbose=2,validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "model.add(Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#model.add(Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(np.array(data), np.array(target), nb_epoch=60, batch_size=34, verbose=1)   #, validation_data=(np.array(x_test), np.array(y_test) ))\n",
    "# predict = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('I:/NLP/data_train/train/quesmodel-weight-gach.h')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('I:/NLP/data_train/train/quesmodel_architecture-gach.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "#model.save('I:/NLP/data_train/train/quesmodel-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from keras.models import load_model\n",
    "\n",
    "#import sentimential as st\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('I:/NLP/data_train/train/quesmodel_architecture.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('I:/NLP/data_train/train/quesmodel-weight-1.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19899, 300)\tembedding_3_1/embeddings:0\n",
      "(300, 512)\tlstm_3_1/kernel:0\n",
      "(128, 512)\tlstm_3_1/recurrent_kernel:0\n",
      "(512,)\tlstm_3_1/bias:0\n",
      "(128, 1)\tdense_3_1/kernel:0\n",
      "(1,)\tdense_3_1/bias:0\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    for weight in layer.weights:\n",
    "        print(f'{weight.shape}\\t{weight.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 180)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('I:/NLP/data_train/train/quesmodel-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/cc.vi.300.vec', encoding = \"utf8\") as fp:\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10014  10038  14875  10015  10065  10391  10006  10061  10030  10045\n",
      "   10592  48801  10024  10170  12682  10048  10018  10098  10055  10065\n",
      "   10062  10170  12682  10069  13876  11245  10065  10010  11280 786841\n",
      "   10026  12738  13054  10221  10547      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0]]\n",
      "[[1.769223e-05]]\n",
      "cmt\n"
     ]
    }
   ],
   "source": [
    "f = ['Các bạn IU cho mình hỏi là sau khi làm xong thesis ở học kì 2 năm 4 thì mình còn học kì 3 rãnh. Liệu mình có thể đkmh để cải thiện điểm ko?']\n",
    "ids = []\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "split = cleanSentences(line).split()\n",
    "\n",
    "for i, word in enumerate(split):\n",
    "    if i >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        idx = 0\n",
    "        for x in range(2000000):\n",
    "            #line = fp.readline().strip()\n",
    "            doline = lines [x] \n",
    "            #while line:\n",
    "            z = doline.split(' ', 1)[0]\n",
    "            #if x == 200000:\n",
    "            if z == word:\n",
    "                idx = x + 9998\n",
    "                break\n",
    "        #idx = wordsList.index(split[word_idx])\n",
    "        ids.append(idx)\n",
    "        #ids.append(wordsList.index(word))\n",
    "    except ValueError:\n",
    "        ids.append(518835)\n",
    "        \n",
    "ids = np.array(ids + ([0] * (maxSeqLength - len(ids))))\n",
    "ids = np.expand_dims(ids, axis=0)\n",
    "prediction = model.predict(ids)\n",
    "print (ids)\n",
    "print(prediction)\n",
    "if prediction < 0.5: \n",
    "    print ('cmt')\n",
    "else: \n",
    "    print ('ques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.8111539e-06]]\n"
     ]
    }
   ],
   "source": [
    "f = ['Anh_chị cho e hỏi . Chiều_qua e học ở VNU có thấy 1 chị khóa trên mặc sơ_mi trắng cột tóc cao cực xinh']\n",
    "ids = []\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "split = cleanSentences(line).split()\n",
    "\n",
    "for i, word in enumerate(split):\n",
    "    if i >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        ids.append(wordsList.index(word))\n",
    "    except ValueError:\n",
    "        ids.append(0)\n",
    "ids = np.array(ids + ([0] * (maxSeqLength - len(ids))))\n",
    "ids = np.expand_dims(ids, axis=0)\n",
    "prediction = model.predict(ids)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['anh_chị', 'cho', 'e', 'hỏi', 'chiều_qua', 'e', 'học', 'ở', 'vnu', 'có', 'thấy', '1', 'chị', 'khóa', 'trên', 'mặc', 'sơ_mi', 'trắng', 'cột', 'tóc', 'cao', 'cực', 'xinh']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-e45783c52023>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#inputu.append(idx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "f = ['Anh_chị cho e hỏi . Chiều_qua e học ở VNU có thấy 1 chị khóa trên mặc sơ_mi trắng cột tóc cao cực xinh']\n",
    "inputu = []\n",
    "ids = np.zeros((1, 180), dtype=np.int32)\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "cleanedLine = cleanSentences(line)\n",
    "split = cleanedLine.split()\n",
    "print (len(split))\n",
    "print (split)\n",
    "for word_idx in range(len(split)):\n",
    "    if word_idx >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        idx = wordsList.index(split[word_idx])\n",
    "        #inputu.append(idx)\n",
    "        ids[word_idx] = idx \n",
    "    except ValueError:\n",
    "        ids[word_idx] = unk_idx\n",
    "\n",
    "inputu = np.array(inputu)\n",
    "print (inputu)\n",
    "# prediction = model.predict(x_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of line is 9448\n",
      "The total number of words in the files is 49825\n",
      "The average number of words in the files is 5.273602878916173\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "numWords = []\n",
    "qf = len(open('I:/NLP/data_train/xulykogach.txt',  encoding=\"utf8\").readlines())\n",
    "cf = len(open('I:/NLP/data_train/xuly2kogach.txt',  encoding=\"utf8\").readlines())\n",
    "with open('I:/NLP/data_train/xulykogach.txt', \"r\", encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "with open('I:/NLP/data_train/xuly2kogach.txt', \"r\", encoding='utf-8') as f:\n",
    "    for nf in f.readlines():\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of line is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "train = h5py.File('I:/NLP/data_train/cc.vi.300.hdf5', 'w')\n",
    "vectors = train.create_dataset('vectors', (2000000, 300), dtype='float')\n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "words = train.create_dataset('words', (2000000,), dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "buf_size = 10000\n",
    "idx = 0\n",
    "vector_buf, word_buf = [], []\n",
    "with open('E:/cc.vi.300.vec', encoding='utf-8') as f:\n",
    "    line = f.readline().strip()\n",
    "    for i in range(2000000):\n",
    "        tmp = f.readline().strip().split()\n",
    "        vector_buf.append(np.array(tmp[1:], dtype='float32'))\n",
    "        word_buf.append(tmp[0])\n",
    "        if i%buf_size == 0:\n",
    "            next_idx = idx + buf_size\n",
    "            vectors[idx:next_idx] = vector_buf\n",
    "            words[idx:next_idx] = word_buf\n",
    "            vector_buf, word_buf = [], []\n",
    "            idx = next_idx\n",
    "            \n",
    "    train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THẠNH\n",
      "[ 2.22999994e-02  2.69000009e-02  1.89999994e-02  5.29999994e-02\n",
      "  1.59000009e-02 -3.62999998e-02 -2.04000007e-02 -8.99999985e-04\n",
      "  9.60000046e-03 -3.92999984e-02  9.99999978e-03 -1.02000004e-02\n",
      "  2.08999999e-02  1.82000007e-02 -1.78999994e-02 -2.06000004e-02\n",
      " -2.91000009e-02 -1.71000008e-02 -1.95000004e-02 -1.47000002e-02\n",
      " -2.05000006e-02  4.45000008e-02 -2.08999999e-02  1.44999996e-02\n",
      " -1.41000003e-02  5.49999997e-03 -4.49999981e-03 -9.49999969e-03\n",
      "  3.80000006e-03 -1.26000000e-02 -1.20999999e-02  1.00000005e-03\n",
      "  2.82000005e-02 -6.23000003e-02 -1.26999998e-02 -2.89999996e-03\n",
      " -2.34999992e-02  1.52000003e-02  1.85000002e-02 -2.07000002e-02\n",
      "  2.49999994e-03  2.84000002e-02  1.40300006e-01 -3.68999988e-02\n",
      "  1.35000004e-02 -4.49000001e-02 -9.70000029e-03  8.00000038e-03\n",
      " -3.00000003e-03  2.87999995e-02 -9.70000029e-03 -2.60000001e-03\n",
      " -5.35999984e-02  1.78999994e-02  6.99999975e-04 -9.53999981e-02\n",
      " -3.50000001e-02 -3.04000005e-02  1.15000000e-02 -7.60000013e-03\n",
      " -4.10000002e-03 -2.17000004e-02  6.89999992e-03 -1.89999994e-02\n",
      "  1.99999995e-04  1.00000005e-03 -1.99999995e-04  5.09999990e-02\n",
      "  1.31999999e-02 -2.32999995e-02  1.76999997e-02  3.73999998e-02\n",
      " -3.19999992e-03 -2.97999997e-02 -3.29000019e-02 -6.69999979e-03\n",
      "  4.90000006e-03  2.16000006e-02 -9.80000012e-03 -3.50000011e-03\n",
      "  8.60000029e-03 -2.96999998e-02  2.25000009e-02  8.99999961e-03\n",
      " -9.60000046e-03  1.60000008e-02  2.08999999e-02  2.74999999e-02\n",
      "  4.19999985e-03  2.83000004e-02 -8.29999987e-03 -3.03000007e-02\n",
      "  1.48000000e-02  2.07000002e-02  4.32000011e-02 -5.11000007e-02\n",
      "  2.11999994e-02 -1.31999999e-02  6.59999996e-03  7.00000022e-03\n",
      " -9.20000020e-03 -2.20999997e-02 -2.30000005e-03 -2.09999993e-03\n",
      " -1.96000002e-02  2.67999992e-02  9.89999995e-03 -2.96000000e-02\n",
      " -3.48999985e-02  2.74000000e-02  7.69999996e-03  5.10000018e-03\n",
      " -1.09999999e-03  1.00999996e-02 -1.33999996e-02 -1.62000004e-02\n",
      " -1.76999997e-02  1.72000006e-02  3.00000003e-03  8.10000021e-03\n",
      "  2.44999994e-02 -2.59000007e-02 -5.00000007e-02 -3.20000015e-02\n",
      " -1.49999997e-02 -1.76999997e-02 -0.00000000e+00 -9.99999975e-05\n",
      " -2.08000001e-02 -3.57999988e-02  1.75000001e-02 -4.30000015e-03\n",
      " -1.00000005e-03 -1.70000009e-02 -2.51000002e-02 -9.30000003e-03\n",
      " -2.04000007e-02  4.99999989e-03  2.19999999e-03  3.10000009e-03\n",
      " -1.30000003e-02  4.05000001e-02 -9.30000003e-03  8.20000004e-03\n",
      " -4.65000011e-02 -2.85000000e-02  5.70000010e-03 -4.01000008e-02\n",
      " -3.29999998e-03  1.83000006e-02  0.00000000e+00  4.80000023e-03\n",
      "  3.51999998e-02 -7.60000013e-03  2.15000007e-02 -2.42999997e-02\n",
      "  3.99999990e-04 -3.89999989e-03 -1.74000002e-02  1.00000005e-03\n",
      " -1.20999999e-02 -2.30000005e-03  3.02000009e-02 -1.09000001e-02\n",
      " -8.00000038e-03  2.26500005e-01 -4.94000018e-02  1.08000003e-02\n",
      "  5.31999990e-02  4.63000014e-02  2.00999994e-02 -3.61000001e-02\n",
      " -1.31999999e-02 -1.53000001e-02 -3.86999995e-02  2.47000009e-02\n",
      " -2.44999994e-02  1.16999997e-02 -1.93000007e-02 -1.79999992e-02\n",
      " -2.78999992e-02 -1.65999997e-02  7.69999996e-03  1.30000000e-03\n",
      " -3.00000014e-04 -1.14000002e-02 -2.97999997e-02  1.35000004e-02\n",
      "  7.19999988e-03 -6.06999993e-02  1.70000002e-03  9.12000015e-02\n",
      " -1.20000001e-02 -9.80000012e-03 -2.55999994e-02  1.09999999e-03\n",
      "  4.00000019e-03  3.00000014e-04 -3.17000002e-02  4.78999987e-02\n",
      "  3.06000002e-02 -4.30000015e-03 -2.19999999e-03  1.29000004e-02\n",
      " -4.90000006e-03  1.63000003e-02  4.49999981e-03  2.00000009e-03\n",
      " -1.00999996e-02 -3.99999991e-02 -2.47000009e-02 -1.66999996e-02\n",
      "  2.41000000e-02  1.16999997e-02  2.38000005e-02  1.27999997e-02\n",
      " -3.59999994e-03  3.15000005e-02  1.21999998e-02 -2.30000000e-02\n",
      " -4.69999993e-03 -9.60000046e-03  6.30000001e-03 -1.19000003e-02\n",
      " -5.00000024e-04  1.79999992e-02 -1.20000006e-03  1.56999994e-02\n",
      "  2.01999992e-02 -6.20000018e-03 -1.39999995e-03  7.60000013e-03\n",
      " -4.80000023e-03 -3.83000001e-02  1.64999999e-02 -2.89999996e-03\n",
      " -1.26000000e-02 -4.49999981e-03  2.56999992e-02  2.34999992e-02\n",
      "  4.00000019e-03  1.79999992e-02  3.99000011e-02  1.53000001e-02\n",
      "  1.29000004e-02  1.73000004e-02  1.19000003e-02 -7.60000013e-03\n",
      " -1.55999996e-02  2.86999997e-02  1.71000008e-02 -1.42999999e-02\n",
      "  2.96000000e-02 -5.40000014e-03 -1.31999999e-02 -9.39999986e-03\n",
      " -2.40000011e-03 -4.00000019e-03  1.56999994e-02  1.51000004e-02\n",
      "  1.22999996e-02  4.03999984e-02 -2.41999999e-02 -1.93000007e-02\n",
      " -4.39999998e-03  4.06000018e-02 -1.37999998e-02  3.40999998e-02\n",
      " -5.20000001e-03  3.97000015e-02 -3.80000006e-03 -3.99999990e-04\n",
      " -3.59999985e-02 -1.31999999e-02  2.30000005e-03 -4.94999997e-02\n",
      " -1.66999996e-02  1.59000009e-02 -3.29999998e-03  3.09999995e-02\n",
      "  1.36000002e-02 -1.46000003e-02  2.20999997e-02  2.67999992e-02\n",
      " -1.70000002e-03  2.17000004e-02  7.30000017e-03  2.20999997e-02\n",
      " -2.40000011e-03  4.52000014e-02 -2.63999999e-02 -1.59000009e-02\n",
      "  1.25000002e-02  8.50000046e-03 -5.20999990e-02  2.32999995e-02\n",
      " -2.19999999e-02 -3.97000015e-02 -1.26000000e-02  1.21999998e-02]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "t_train = h5py.File('I:/NLP/data_train/cc.vi.300.hdf5')\n",
    "print(t_train['words'][100010])\n",
    "t_train['vectors'][0].shape\n",
    "print(t_train['vectors'][99990])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "wordsList = t_train['words']\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "#wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "# wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = t_train['vectors']\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fileroad = 'E:/cc.vi.300.vec' \n",
    "tmp = np.array\n",
    "\n",
    "tuw = []\n",
    "#y = 0 \n",
    "with open(fileroad, encoding=\"utf8\") as fp:\n",
    "    for x in range(2000000):\n",
    "        line = fp.readline().strip()\n",
    "        #while line:\n",
    "        z = line.split(' ', 1)[0]\n",
    "        #if x == 19970:\n",
    "        if z == 'cám':\n",
    "            print ('index is: ' + str(x))\n",
    "            print (len(line.split()[1:]))\n",
    "            tmp = np.array(line.split()[1:], dtype='float32')\n",
    "            print(tmp)\n",
    "            # print (line.split(' ', 1)[1:])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1896\n",
      "7552\n",
      "9448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "maxSeqLength = 180\n",
    "\n",
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "\n",
    "fileroad = 'E:/cc.vi.300.vec'\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype=np.int32)\n",
    "nFiles = 0\n",
    "unk_idx = 518835\n",
    "cmtFiles = open('I:/NLP/data_train/xuly2kogach.txt', encoding='utf-8')\n",
    "questionFiles = open('I:/NLP/data_train/xulykogach.txt', encoding='utf-8')\n",
    "count = len(questionFiles.readlines())\n",
    "count2 = len(cmtFiles.readlines())\n",
    "print (count)\n",
    "with open(fileroad, encoding=\"utf8\") as fp:\n",
    "    lines = fp.readlines()\n",
    "    with open('I:/NLP/data_train/xulykogach.txt', 'r', encoding='utf-8') as f:\n",
    "        for qf_idx in range(count):\n",
    "            line = f.readline()\n",
    "            #cleanedLine = cleanSentences(line)\n",
    "            split = line.split()\n",
    "            #print (len(split))\n",
    "            for word_idx in range(len(split)):\n",
    "                if word_idx >= maxSeqLength:\n",
    "                    break\n",
    "                try:\n",
    "                    for x in range(2000000):\n",
    "                        #line = fp.readline().strip()\n",
    "                        doline = lines [x] \n",
    "                        #while line:\n",
    "                        z = doline.split(' ', 1)[0]\n",
    "                        #if x == 200000:\n",
    "                        if z == split[word_idx]:\n",
    "                            idx = x + 9998\n",
    "                            #print ('index is: ' + str(x))\n",
    "                            #print (line)\n",
    "                            #print (line.split(' ', 1)[1])\n",
    "                            break\n",
    "                    #idx = wordsList.index(split[word_idx])\n",
    "                    ids[qf_idx, word_idx] = idx\n",
    "                except ValueError:\n",
    "                    ids[qf_idx, word_idx] = unk_idx\n",
    "            #print (line)\n",
    "\n",
    "    print (count2)\n",
    "    print (count+count2)\n",
    "    with open('I:/NLP/data_train/xuly2kogach.txt', 'r', encoding='utf-8') as f:\n",
    "        for cf_idx in range(count, count+count2-2):\n",
    "            line = f.readline()\n",
    "            cleanedLine = cleanSentences(line)\n",
    "            split = cleanedLine.split()\n",
    "            for word_idx in range(len(split)):\n",
    "                if word_idx >= maxSeqLength:\n",
    "                    break\n",
    "                try:\n",
    "                    for x in range(2000000):\n",
    "                        doline = lines[x]\n",
    "                        z = doline.split(' ', 1)[0]\n",
    "                        if z == split[word_idx]:\n",
    "                            idx = x + 9998\n",
    "                            break\n",
    "                    #idx = wordsList.index(split[word_idx])\n",
    "                    ids[cf_idx, word_idx] = idx\n",
    "                except ValueError:\n",
    "                    ids[cf_idx, word_idx] = unk_idx\n",
    "                \n",
    "np.save('I:/NLP/data_train/idsMatrixques-2.npy', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 4136: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-6b7fe031b79a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;31m#while line:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\lib\\encodings\\cp1258.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 4136: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fileroad ='E:\\cc.vi.300.vec'\n",
    "filepath =\"I:/NLP/data_train/tu.npy\"\n",
    "tuw = []\n",
    "y = 0 \n",
    "with open(fileroad) as fp:\n",
    "    with open(filepath, 'wb') as tu:\n",
    "        for x in range(2000000):\n",
    "            line = fp.readline()\n",
    "            #while line:\n",
    "            z = line.split(' ', 1)[0]\n",
    "            if z == 'ngon':\n",
    "                print (x)\n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "            #y += 1\n",
    "            #if y % 5 == 0:\n",
    "                #np.save(tu, tuw)\n",
    "                #y = 0\n",
    "                #tuw = []\n",
    "            #print(\"Line {}: {}\".format(x, line.split(' ', 1)[0]))\n",
    "            #line = fp.readline()\n",
    "        \n",
    "# with open(filename, 'rb') as fff:\n",
    "#      print(fff.readlines())  \n",
    "\n",
    "#wordsList = np.load(f'/Users/mac/Desktop/NLP/data_train/tu.npy')\n",
    "#print('Simplified vocabulary loaded!')\n",
    "#wordsList = wordsList.tolist()\n",
    "#print('Size of the vocabulary: ', len(wordsList)) \n",
    "#print (wordsList)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import numpy as np\n",
    "\n",
    "filename = 'outarray.h5'\n",
    "ROW_SIZE = 100\n",
    "NUM_COLUMNS = 2000\n",
    "\n",
    "f = tables.open_file(filename, mode='w')\n",
    "atom = tables.Float64Atom()\n",
    "\n",
    "array_c = f.create_earray(f.root, 'data', atom, (0, ROW_SIZE))\n",
    "\n",
    "for idx in range(NUM_COLUMNS):\n",
    "    x = np.random.rand(1, ROW_SIZE)\n",
    "    array_c.append(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tables.open_file(filename, mode='a')\n",
    "f.root.data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-45-96c1f28f7273>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-96c1f28f7273>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(f.root.data[1:10,2:20]\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "f = tables.open_file(filename, mode='r')\n",
    "print(f.root.data[1:10,2:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71101844]\n",
      " [0.7659625 ]\n",
      " [0.6447913 ]\n",
      " [0.71104187]\n",
      " [0.76595825]\n",
      " [0.7110288 ]\n",
      " [0.0194401 ]\n",
      " [0.76595813]\n",
      " [0.76596576]\n",
      " [0.67143285]\n",
      " [0.71099645]\n",
      " [0.64479125]\n",
      " [0.64479136]\n",
      " [0.7659696 ]\n",
      " [0.3881391 ]\n",
      " [0.71102977]\n",
      " [0.7110287 ]\n",
      " [0.765958  ]\n",
      " [0.2981338 ]\n",
      " [0.71102875]\n",
      " [0.39108372]\n",
      " [0.7110285 ]\n",
      " [0.71102786]\n",
      " [0.7109137 ]\n",
      " [0.71102864]\n",
      " [0.7110284 ]\n",
      " [0.3830236 ]\n",
      " [0.7537285 ]\n",
      " [0.6448196 ]\n",
      " [0.76596457]\n",
      " [0.71102756]\n",
      " [0.38088366]\n",
      " [0.7168713 ]\n",
      " [0.64479214]\n",
      " [0.7108114 ]\n",
      " [0.73556507]\n",
      " [0.64479125]\n",
      " [0.6725571 ]\n",
      " [0.7565845 ]\n",
      " [0.74750835]\n",
      " [0.6447913 ]\n",
      " [0.3809575 ]\n",
      " [0.7652571 ]\n",
      " [0.7659657 ]\n",
      " [0.64479125]\n",
      " [0.7659617 ]\n",
      " [0.7658632 ]\n",
      " [0.64479136]\n",
      " [0.71102804]\n",
      " [0.76595896]\n",
      " [0.7659613 ]\n",
      " [0.71101564]\n",
      " [0.7280378 ]\n",
      " [0.7110273 ]\n",
      " [0.7659293 ]\n",
      " [0.64479154]\n",
      " [0.7109196 ]\n",
      " [0.727397  ]\n",
      " [0.64479125]\n",
      " [0.7110287 ]\n",
      " [0.5237914 ]\n",
      " [0.75696695]\n",
      " [0.7655316 ]\n",
      " [0.76596934]\n",
      " [0.7108184 ]\n",
      " [0.64618635]\n",
      " [0.6447914 ]\n",
      " [0.63785267]\n",
      " [0.3818133 ]\n",
      " [0.64480066]\n",
      " [0.71582514]\n",
      " [0.6447913 ]\n",
      " [0.7110287 ]\n",
      " [0.7110284 ]\n",
      " [0.64479125]\n",
      " [0.71102893]\n",
      " [0.71102875]\n",
      " [0.64479166]\n",
      " [0.71102464]\n",
      " [0.38353854]\n",
      " [0.64479136]\n",
      " [0.64479136]\n",
      " [0.6447913 ]\n",
      " [0.64479125]\n",
      " [0.71102816]\n",
      " [0.7055075 ]\n",
      " [0.71775556]\n",
      " [0.7110284 ]\n",
      " [0.3760789 ]\n",
      " [0.76463705]\n",
      " [0.7107066 ]\n",
      " [0.64479125]\n",
      " [0.71102864]\n",
      " [0.38136715]\n",
      " [0.64479125]\n",
      " [0.76569855]\n",
      " [0.7659613 ]\n",
      " [0.0198444 ]\n",
      " [0.71102875]\n",
      " [0.7110316 ]\n",
      " [0.7110285 ]\n",
      " [0.7110275 ]\n",
      " [0.6447913 ]\n",
      " [0.38088632]\n",
      " [0.71102685]\n",
      " [0.64479125]\n",
      " [0.6947513 ]\n",
      " [0.71102875]\n",
      " [0.7659621 ]\n",
      " [0.7110811 ]\n",
      " [0.76536435]\n",
      " [0.7659593 ]\n",
      " [0.7110283 ]\n",
      " [0.6447915 ]\n",
      " [0.7655344 ]\n",
      " [0.7050204 ]\n",
      " [0.7110285 ]\n",
      " [0.71102685]\n",
      " [0.7110287 ]\n",
      " [0.65077466]\n",
      " [0.7110288 ]\n",
      " [0.7185234 ]\n",
      " [0.7645583 ]\n",
      " [0.37232107]\n",
      " [0.76595837]\n",
      " [0.71102875]\n",
      " [0.38172424]\n",
      " [0.76585674]\n",
      " [0.39954254]\n",
      " [0.5323426 ]\n",
      " [0.7110283 ]\n",
      " [0.76595956]\n",
      " [0.48849908]\n",
      " [0.64479125]\n",
      " [0.6447913 ]\n",
      " [0.7659703 ]\n",
      " [0.76595855]\n",
      " [0.71102965]\n",
      " [0.7110274 ]\n",
      " [0.7630909 ]\n",
      " [0.76595825]\n",
      " [0.71212864]\n",
      " [0.7111829 ]\n",
      " [0.76596314]\n",
      " [0.64479125]\n",
      " [0.6798715 ]\n",
      " [0.7110278 ]\n",
      " [0.7659588 ]\n",
      " [0.53521067]\n",
      " [0.6454551 ]\n",
      " [0.2572882 ]\n",
      " [0.64479125]\n",
      " [0.3963914 ]\n",
      " [0.6601735 ]\n",
      " [0.7106486 ]\n",
      " [0.6447921 ]\n",
      " [0.71106625]\n",
      " [0.67308074]\n",
      " [0.7659619 ]\n",
      " [0.7142413 ]\n",
      " [0.64479125]\n",
      " [0.52562207]\n",
      " [0.7659597 ]\n",
      " [0.7110284 ]\n",
      " [0.08699465]\n",
      " [0.64479125]\n",
      " [0.64479136]\n",
      " [0.7659738 ]\n",
      " [0.65152526]\n",
      " [0.64479125]\n",
      " [0.7659591 ]\n",
      " [0.71102893]\n",
      " [0.76586264]\n",
      " [0.71102864]\n",
      " [0.76596475]\n",
      " [0.66771007]\n",
      " [0.765968  ]\n",
      " [0.6447913 ]\n",
      " [0.64479136]\n",
      " [0.5258447 ]\n",
      " [0.5328569 ]\n",
      " [0.71109766]\n",
      " [0.64479125]\n",
      " [0.76454985]\n",
      " [0.3836958 ]\n",
      " [0.65889657]\n",
      " [0.76596403]\n",
      " [0.64479125]\n",
      " [0.70972353]\n",
      " [0.64479125]\n",
      " [0.71119887]\n",
      " [0.3872057 ]\n",
      " [0.38086286]\n",
      " [0.76595676]\n",
      " [0.75104064]\n",
      " [0.6447913 ]\n",
      " [0.3818298 ]\n",
      " [0.7659537 ]\n",
      " [0.3762189 ]\n",
      " [0.01844043]\n",
      " [0.38129595]\n",
      " [0.01975161]\n",
      " [0.01849633]\n",
      " [0.32194775]\n",
      " [0.5716783 ]\n",
      " [0.3818937 ]\n",
      " [0.01809639]\n",
      " [0.02105428]\n",
      " [0.01940688]\n",
      " [0.3810515 ]\n",
      " [0.38186955]\n",
      " [0.3808937 ]\n",
      " [0.38107345]\n",
      " [0.7116189 ]\n",
      " [0.3818804 ]\n",
      " [0.38120976]\n",
      " [0.6448051 ]\n",
      " [0.38138732]\n",
      " [0.37510026]\n",
      " [0.71101356]\n",
      " [0.01930059]\n",
      " [0.533384  ]\n",
      " [0.01908697]\n",
      " [0.01893645]\n",
      " [0.01878231]\n",
      " [0.37236124]\n",
      " [0.38179636]\n",
      " [0.3944629 ]\n",
      " [0.3809401 ]\n",
      " [0.711016  ]\n",
      " [0.01933908]\n",
      " [0.66696024]\n",
      " [0.02143335]\n",
      " [0.3761796 ]\n",
      " [0.01824029]\n",
      " [0.38221428]\n",
      " [0.35625023]\n",
      " [0.01874955]\n",
      " [0.35647488]\n",
      " [0.5366374 ]\n",
      " [0.3813797 ]\n",
      " [0.71102816]\n",
      " [0.71056056]\n",
      " [0.7110251 ]\n",
      " [0.3809906 ]\n",
      " [0.5358665 ]\n",
      " [0.3755852 ]\n",
      " [0.01966306]\n",
      " [0.36060855]\n",
      " [0.38138932]\n",
      " [0.37613913]\n",
      " [0.38115096]\n",
      " [0.3503273 ]\n",
      " [0.01790009]\n",
      " [0.71102697]\n",
      " [0.38085893]\n",
      " [0.684708  ]\n",
      " [0.37700438]\n",
      " [0.3816944 ]\n",
      " [0.06148223]\n",
      " [0.7110265 ]\n",
      " [0.01818929]\n",
      " [0.76595837]\n",
      " [0.376401  ]\n",
      " [0.01893343]\n",
      " [0.6728254 ]\n",
      " [0.710992  ]\n",
      " [0.76596135]\n",
      " [0.01832657]\n",
      " [0.37624654]\n",
      " [0.02317768]\n",
      " [0.0694463 ]\n",
      " [0.01823881]\n",
      " [0.37251425]\n",
      " [0.66855246]\n",
      " [0.7108109 ]\n",
      " [0.38109264]\n",
      " [0.01931167]\n",
      " [0.36693898]\n",
      " [0.38087067]\n",
      " [0.7081833 ]\n",
      " [0.3995474 ]\n",
      " [0.381822  ]\n",
      " [0.6024816 ]\n",
      " [0.71102864]\n",
      " [0.01830771]\n",
      " [0.02088621]\n",
      " [0.02036238]\n",
      " [0.6424311 ]\n",
      " [0.3167428 ]\n",
      " [0.38108647]\n",
      " [0.67131096]\n",
      " [0.31292382]\n",
      " [0.7110283 ]\n",
      " [0.37446266]\n",
      " [0.6686577 ]\n",
      " [0.64986956]\n",
      " [0.0188095 ]\n",
      " [0.71102154]\n",
      " [0.01785084]\n",
      " [0.3763984 ]\n",
      " [0.01811012]\n",
      " [0.02442878]\n",
      " [0.02045334]\n",
      " [0.0177253 ]\n",
      " [0.37678257]\n",
      " [0.7644064 ]\n",
      " [0.71101344]\n",
      " [0.35393107]\n",
      " [0.33874184]\n",
      " [0.3855664 ]\n",
      " [0.36865038]\n",
      " [0.01916495]\n",
      " [0.38167113]\n",
      " [0.30392012]\n",
      " [0.01995525]\n",
      " [0.02188025]\n",
      " [0.3808937 ]\n",
      " [0.3462126 ]\n",
      " [0.02163799]\n",
      " [0.38175446]\n",
      " [0.67840016]\n",
      " [0.38176143]\n",
      " [0.01948889]\n",
      " [0.6696814 ]\n",
      " [0.53237414]\n",
      " [0.5302977 ]\n",
      " [0.64479554]\n",
      " [0.01951607]\n",
      " [0.53427804]\n",
      " [0.7659498 ]\n",
      " [0.71102864]\n",
      " [0.5343199 ]\n",
      " [0.38089785]\n",
      " [0.5040292 ]\n",
      " [0.30658245]\n",
      " [0.37795526]\n",
      " [0.6709107 ]\n",
      " [0.3694451 ]\n",
      " [0.38104805]\n",
      " [0.64479125]\n",
      " [0.64479274]\n",
      " [0.76585394]\n",
      " [0.6447913 ]\n",
      " [0.64479125]\n",
      " [0.71102774]\n",
      " [0.6057516 ]\n",
      " [0.3809485 ]\n",
      " [0.76596284]\n",
      " [0.6377617 ]\n",
      " [0.7110259 ]\n",
      " [0.7659587 ]\n",
      " [0.01852655]\n",
      " [0.3381152 ]\n",
      " [0.37981963]\n",
      " [0.38138396]\n",
      " [0.7110284 ]\n",
      " [0.0189647 ]\n",
      " [0.71069396]\n",
      " [0.01918176]\n",
      " [0.765947  ]\n",
      " [0.7110256 ]\n",
      " [0.71102875]\n",
      " [0.51459664]\n",
      " [0.64479136]\n",
      " [0.38147494]\n",
      " [0.51431096]\n",
      " [0.7109805 ]\n",
      " [0.01773175]\n",
      " [0.7659592 ]\n",
      " [0.76596195]\n",
      " [0.37625983]\n",
      " [0.31095222]\n",
      " [0.7110285 ]\n",
      " [0.380861  ]\n",
      " [0.7096968 ]\n",
      " [0.7103812 ]\n",
      " [0.7086484 ]\n",
      " [0.38095072]\n",
      " [0.71102846]\n",
      " [0.7110271 ]\n",
      " [0.7109956 ]\n",
      " [0.4089499 ]\n",
      " [0.38092774]\n",
      " [0.38087407]\n",
      " [0.2979263 ]\n",
      " [0.38097113]\n",
      " [0.64479196]\n",
      " [0.01952618]\n",
      " [0.71102864]\n",
      " [0.7110287 ]\n",
      " [0.6447913 ]\n",
      " [0.2053827 ]\n",
      " [0.6708279 ]\n",
      " [0.38090298]\n",
      " [0.6710728 ]\n",
      " [0.61467415]\n",
      " [0.71102893]\n",
      " [0.7106356 ]\n",
      " [0.7110246 ]\n",
      " [0.71102864]\n",
      " [0.38250342]\n",
      " [0.7658998 ]\n",
      " [0.3818631 ]\n",
      " [0.38099018]\n",
      " [0.01869628]\n",
      " [0.01833772]\n",
      " [0.71103024]\n",
      " [0.7110268 ]\n",
      " [0.7611207 ]\n",
      " [0.09176175]\n",
      " [0.38116583]\n",
      " [0.38173088]\n",
      " [0.7110264 ]\n",
      " [0.7106791 ]\n",
      " [0.64479136]\n",
      " [0.3751341 ]\n",
      " [0.7563828 ]\n",
      " [0.01940928]\n",
      " [0.26339382]\n",
      " [0.7110055 ]\n",
      " [0.71075135]\n",
      " [0.7108626 ]\n",
      " [0.7110281 ]\n",
      " [0.71102756]\n",
      " [0.34099618]\n",
      " [0.76597124]\n",
      " [0.3444845 ]\n",
      " [0.7659236 ]\n",
      " [0.3818845 ]\n",
      " [0.38085756]\n",
      " [0.38086358]\n",
      " [0.67357916]\n",
      " [0.38093957]\n",
      " [0.7659587 ]\n",
      " [0.71102154]\n",
      " [0.7109259 ]\n",
      " [0.3662871 ]\n",
      " [0.53459233]\n",
      " [0.7110279 ]\n",
      " [0.01853682]\n",
      " [0.7659724 ]\n",
      " [0.64479154]\n",
      " [0.02140156]\n",
      " [0.7659713 ]\n",
      " [0.38092172]\n",
      " [0.7658056 ]\n",
      " [0.76595736]\n",
      " [0.7174833 ]\n",
      " [0.01780939]\n",
      " [0.5271319 ]\n",
      " [0.6667717 ]\n",
      " [0.02052858]\n",
      " [0.5257096 ]\n",
      " [0.38087198]\n",
      " [0.07203196]\n",
      " [0.01897575]\n",
      " [0.71147287]\n",
      " [0.67612255]\n",
      " [0.71102864]\n",
      " [0.02145646]\n",
      " [0.6447913 ]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(np.array(x_test))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((1, 180), dtype=np.int32)\n",
    "ids[1] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",validation_data=(np.array(x_test), np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
