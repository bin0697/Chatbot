{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8bc0b3a1fe89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"Neutral\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-z0-9\\s]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Positive'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['sentiment'] == 'Positive'].size)\n",
    "print(data[ data['sentiment'] == 'Negative'].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "#X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'E:\\cc.vi.300.vec'  \n",
    "with open(filepath) as fp:\n",
    "    for x in range(10):\n",
    "        line = fp.readline()\n",
    "        #print (line.split(' ', 1)[0])\n",
    "        cnt = 1\n",
    "        #while line:\n",
    "        print(\"Line {}: {}\".format(x, line.split(' ', 1)[0]))\n",
    "        #line = fp.readline()\n",
    "        cnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Transforms each text in texts to a sequence of integers.\n",
       "\n",
       "Only top `num_words-1` most frequent words will be taken into account.\n",
       "Only words known by the tokenizer will be taken into account.\n",
       "\n",
       "# Arguments\n",
       "    texts: A list of texts (strings).\n",
       "\n",
       "# Returns\n",
       "    A list of sequences.\n",
       "\u001b[1;31mFile:\u001b[0m      i:\\anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tokenizer.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['  scottwalker didnt catch the full gopdebate last night here are some of scotts best lines in 90 seconds walker16 httptcozsff',\n",
       "       '  robgeorge that carly fiorina is trending  hours after her debate  above any of the men in justcompleted gopdebate says shes on ',\n",
       "       '  danscavino gopdebate w realdonaldtrump delivered the highest ratings in the history of presidential debates trump2016 httptco',\n",
       "       ...,\n",
       "       '  lrihendry tedcruz as president i will always tell the truth and do what i said i would do  gopdebates',\n",
       "       '  jrehling gopdebate donald trump says that he doesnt have time for political correctness how does calling women fat pigs save him ',\n",
       "       '  lrihendry tedcruz headed into the presidential debates go ted \\n\\ngopdebates httptco8s67pz8a4a'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92f4bac7d515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "ids = np.load(os.path.join('I:/NLP/data_train/','idsMatrix.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40050, 180)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19899, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors = np.load(f'I:/NLP/data_train/wordVectors.npy')\n",
    "wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a165c5f01a01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "model.add(Embedding(len(wordVectors)+1, 128, weights=wordVectors, input_length=180, trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# LƯU Ý: CẦN PHẢI CHỈNH LẠI ĐƯỜNG DẪN NÀY THÀNH THƯ MỤC CHỨA CÁC FILE ASSIGNMENT3\n",
    "# CHỮ 'drive' có nghĩa là thư mục mặc định của Google drive\n",
    "currentDir = 'I:/NLP/data_train/'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "#wordsList = np.load(f'{currentDir}wordsList.npy')\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "#wordVectors = np.load(f'{currentDir}wordVectors.npy')\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sentence = 'Món này ăn hoài không biết chán'\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
    "word_in_sentence = [word.lower() for word in sentence.split()]\n",
    "for i in range(len(word_in_sentence)):\n",
    "    idx =  wordsList.index(word_in_sentence[i])\n",
    "    sentenceIndexes[i] = idx\n",
    "    \n",
    "# Các chỉ số 7, 8, 9 của sentenceIndexes  vẫn được gán bằng 0 như cũ\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma trận biểu diễn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 40050\n",
      "The total number of words in the files is 2375531\n",
      "The average number of words in the files is 59.31413233458177\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "positiveFiles = ['I:/NLP/data_train/train/pos/' + f for f in listdir('I:/NLP/data_train/train/pos/') if isfile(join('I:/NLP/data_train/train/pos/', f))]\n",
    "negativeFiles = ['I:/NLP/data_train/train/neg/' + f for f in listdir('I:/NLP/data_train/train/neg/') if isfile(join('I:/NLP/data_train/train/neg/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821\n",
      " 14794  8884  6443  5767  8589 18850 15570  5596   799 11060  4222 16893\n",
      " 13078  8136  3364  4454  4756 10304  8885  3553  9782  1232 14359 10606\n",
      "   579 15522  2219 15092 14855 15253  4884  3364  5519  4558  9649   269\n",
      " 15522 12309 14855 11503  2212  4884  7155 11577  4222  5767 15076 12225\n",
      " 10774  1218  2876 19584  4558  2974 13452  5013   842 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  4135  3553 14967  4964\n",
      " 15385  9673  2997 14855  7446  8038 11440  1345   842  5767   803 11060\n",
      " 18791  5013     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "#ids = np.load(f'{currentDir}idsMatrix.npy')\n",
    "print('Word indexes of the first review: ', ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.zeros((numFiles, 1), dtype=np.int32)\n",
    "for pf_idx in range(len(positiveFiles)):\n",
    "    target[pf_idx] = 1\n",
    "for nf_idx in range(len(positiveFiles), len(positiveFiles)+len(negativeFiles)):\n",
    "    target[pf_idx] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.zeros((numFiles, 180), dtype=np.int32)\n",
    "maxSeqLength = 180\n",
    "nFiles = 0\n",
    "unk_idx = wordsList.index('UNK')\n",
    "#data = np.zeros((40050 + 1, 180))\n",
    "target = np.zeros((numFiles, 1), dtype=np.int32)\n",
    "x_test = []\n",
    "y_test = []\n",
    "for pf_idx in range(len(positiveFiles)):\n",
    "    with open(positiveFiles[pf_idx], 'r', encoding='utf-8') as f:\n",
    "        # nIndexes = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word_idx in range(len(split)):\n",
    "            if word_idx >= maxSeqLength:\n",
    "                break\n",
    "            try:\n",
    "                idx = wordsList.index(split[word_idx])\n",
    "                data[pf_idx, word_idx] = idx\n",
    "                \n",
    "            except ValueError:\n",
    "                data[pf_idx, word_idx] = unk_idx\n",
    "        target[pf_idx] = 1\n",
    "\n",
    "for nf_idx in range(len(positiveFiles), len(positiveFiles)+len(negativeFiles)):\n",
    "    with open(negativeFiles[nf_idx-len(positiveFiles)], 'r', encoding='utf-8') as f:\n",
    "        # nIndexes = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word_idx in range(len(split)):\n",
    "            if word_idx >= maxSeqLength:\n",
    "                break\n",
    "            try:\n",
    "                idx = wordsList.index(split[word_idx])\n",
    "                data[nf_idx, word_idx] = idx\n",
    "            except ValueError:\n",
    "                data[nf_idx, word_idx] = unk_idx\n",
    "        target[pf_idx] = 0\n",
    "        \n",
    "\n",
    "#np.save('idsMatrix.npy', ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821\n",
      " 14794  8884  6443  5767  8589 18850 15570  5596   799 11060  4222 16893\n",
      " 13078  8136  3364  4454  4756 10304  8885  3553  9782  1232 14359 10606\n",
      "   579 15522  2219 15092 14855 15253  4884  3364  5519  4558  9649   269\n",
      " 15522 12309 14855 11503  2212  4884  7155 11577  4222  5767 15076 12225\n",
      " 10774  1218  2876 19584  4558  2974 13452  5013   842 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  4135  3553 14967  4964\n",
      " 15385  9673  2997 14855  7446  8038 11440  1345   842  5767   803 11060\n",
      " 18791  5013     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#data = np.array(data)\n",
    "#target = np.array(target)\n",
    "#data = np.zeros((40050 + 1, 180))\n",
    "\n",
    "print (ids[0])\n",
    "print (target[40049])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40050\n",
      "40050\n"
     ]
    }
   ],
   "source": [
    "print (len(ids))\n",
    "print (len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19899, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 300)          5969700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,189,477\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 5,969,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "40050/40050 [==============================] - 177s 4ms/step - loss: 0.6940 - acc: 0.5051\n",
      "Epoch 2/60\n",
      "40050/40050 [==============================] - 175s 4ms/step - loss: 0.6921 - acc: 0.5052\n",
      "Epoch 3/60\n",
      "40050/40050 [==============================] - 179s 4ms/step - loss: 0.6902 - acc: 0.5196\n",
      "Epoch 4/60\n",
      "40050/40050 [==============================] - 175s 4ms/step - loss: 0.6219 - acc: 0.6465\n",
      "Epoch 5/60\n",
      "40050/40050 [==============================] - 175s 4ms/step - loss: 0.4794 - acc: 0.7723\n",
      "Epoch 6/60\n",
      "40050/40050 [==============================] - 176s 4ms/step - loss: 0.4434 - acc: 0.7906\n",
      "Epoch 7/60\n",
      "40050/40050 [==============================] - 176s 4ms/step - loss: 0.4233 - acc: 0.8046\n",
      "Epoch 8/60\n",
      "40050/40050 [==============================] - 177s 4ms/step - loss: 0.4109 - acc: 0.8130\n",
      "Epoch 9/60\n",
      "40050/40050 [==============================] - 213s 5ms/step - loss: 0.4003 - acc: 0.8163\n",
      "Epoch 10/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.3903 - acc: 0.8231\n",
      "Epoch 11/60\n",
      "40050/40050 [==============================] - 178s 4ms/step - loss: 0.3789 - acc: 0.8279\n",
      "Epoch 12/60\n",
      "40050/40050 [==============================] - 178s 4ms/step - loss: 0.3705 - acc: 0.8318\n",
      "Epoch 13/60\n",
      "40050/40050 [==============================] - 183s 5ms/step - loss: 0.3621 - acc: 0.8366\n",
      "Epoch 14/60\n",
      "40050/40050 [==============================] - 218s 5ms/step - loss: 0.3535 - acc: 0.8422\n",
      "Epoch 15/60\n",
      "40050/40050 [==============================] - 187s 5ms/step - loss: 0.3456 - acc: 0.8458\n",
      "Epoch 16/60\n",
      "40050/40050 [==============================] - 186s 5ms/step - loss: 0.3428 - acc: 0.8476\n",
      "Epoch 17/60\n",
      "40050/40050 [==============================] - 186s 5ms/step - loss: 0.3308 - acc: 0.8539\n",
      "Epoch 18/60\n",
      "40050/40050 [==============================] - 188s 5ms/step - loss: 0.3296 - acc: 0.8567\n",
      "Epoch 19/60\n",
      "40050/40050 [==============================] - 198s 5ms/step - loss: 0.3226 - acc: 0.8584\n",
      "Epoch 20/60\n",
      "40050/40050 [==============================] - 265s 7ms/step - loss: 0.3157 - acc: 0.8624\n",
      "Epoch 21/60\n",
      "40050/40050 [==============================] - 214s 5ms/step - loss: 0.3069 - acc: 0.8668\n",
      "Epoch 22/60\n",
      "40050/40050 [==============================] - 192s 5ms/step - loss: 0.3026 - acc: 0.8691\n",
      "Epoch 23/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.2926 - acc: 0.8731\n",
      "Epoch 24/60\n",
      "40050/40050 [==============================] - 184s 5ms/step - loss: 0.2862 - acc: 0.8758\n",
      "Epoch 25/60\n",
      "40050/40050 [==============================] - 188s 5ms/step - loss: 0.2779 - acc: 0.8817\n",
      "Epoch 26/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.2741 - acc: 0.8834\n",
      "Epoch 27/60\n",
      "40050/40050 [==============================] - 185s 5ms/step - loss: 0.2679 - acc: 0.8866\n",
      "Epoch 28/60\n",
      "40050/40050 [==============================] - 184s 5ms/step - loss: 0.2647 - acc: 0.8867\n",
      "Epoch 29/60\n",
      "40050/40050 [==============================] - 184s 5ms/step - loss: 0.2578 - acc: 0.8907\n",
      "Epoch 30/60\n",
      "40050/40050 [==============================] - 188s 5ms/step - loss: 0.2497 - acc: 0.8935\n",
      "Epoch 31/60\n",
      "40050/40050 [==============================] - 194s 5ms/step - loss: 0.2432 - acc: 0.8972\n",
      "Epoch 32/60\n",
      "40050/40050 [==============================] - 196s 5ms/step - loss: 0.2378 - acc: 0.9009\n",
      "Epoch 33/60\n",
      "40050/40050 [==============================] - 196s 5ms/step - loss: 0.2379 - acc: 0.9002\n",
      "Epoch 34/60\n",
      "40050/40050 [==============================] - 195s 5ms/step - loss: 0.2288 - acc: 0.9058\n",
      "Epoch 35/60\n",
      "40050/40050 [==============================] - 194s 5ms/step - loss: 0.2231 - acc: 0.9059\n",
      "Epoch 36/60\n",
      "40050/40050 [==============================] - 195s 5ms/step - loss: 0.2205 - acc: 0.9085\n",
      "Epoch 37/60\n",
      "40050/40050 [==============================] - 190s 5ms/step - loss: 0.2182 - acc: 0.9087\n",
      "Epoch 38/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.2111 - acc: 0.9129\n",
      "Epoch 39/60\n",
      "40050/40050 [==============================] - 190s 5ms/step - loss: 0.2061 - acc: 0.9146\n",
      "Epoch 40/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.1999 - acc: 0.9188\n",
      "Epoch 41/60\n",
      "40050/40050 [==============================] - 189s 5ms/step - loss: 0.1995 - acc: 0.9197\n",
      "Epoch 42/60\n",
      "40050/40050 [==============================] - 191s 5ms/step - loss: 0.1942 - acc: 0.9196\n",
      "Epoch 43/60\n",
      "40050/40050 [==============================] - 195s 5ms/step - loss: 0.1930 - acc: 0.9209\n",
      "Epoch 44/60\n",
      "40050/40050 [==============================] - 192s 5ms/step - loss: 0.1868 - acc: 0.9241\n",
      "Epoch 45/60\n",
      "40050/40050 [==============================] - 193s 5ms/step - loss: 0.1871 - acc: 0.9238\n",
      "Epoch 46/60\n",
      "40050/40050 [==============================] - 192s 5ms/step - loss: 0.1790 - acc: 0.9273\n",
      "Epoch 47/60\n",
      "40050/40050 [==============================] - 193s 5ms/step - loss: 0.1775 - acc: 0.9271\n",
      "Epoch 48/60\n",
      "40050/40050 [==============================] - 192s 5ms/step - loss: 0.1751 - acc: 0.9292\n",
      "Epoch 49/60\n",
      "40050/40050 [==============================] - 192s 5ms/step - loss: 0.1718 - acc: 0.9314\n",
      "Epoch 50/60\n",
      "40050/40050 [==============================] - 194s 5ms/step - loss: 0.1645 - acc: 0.9334\n",
      "Epoch 51/60\n",
      "40050/40050 [==============================] - 186s 5ms/step - loss: 0.1666 - acc: 0.9328\n",
      "Epoch 52/60\n",
      "40050/40050 [==============================] - 185s 5ms/step - loss: 0.1621 - acc: 0.9343\n",
      "Epoch 53/60\n",
      "40050/40050 [==============================] - 185s 5ms/step - loss: 0.1610 - acc: 0.9343\n",
      "Epoch 54/60\n",
      "40050/40050 [==============================] - 187s 5ms/step - loss: 0.1569 - acc: 0.9376\n",
      "Epoch 55/60\n",
      "40050/40050 [==============================] - 188s 5ms/step - loss: 0.1558 - acc: 0.9366\n",
      "Epoch 56/60\n",
      "40050/40050 [==============================] - 188s 5ms/step - loss: 0.1509 - acc: 0.9403\n",
      "Epoch 57/60\n",
      "40050/40050 [==============================] - 187s 5ms/step - loss: 0.1486 - acc: 0.9405\n",
      "Epoch 58/60\n",
      "40050/40050 [==============================] - 187s 5ms/step - loss: 0.1455 - acc: 0.9412\n",
      "Epoch 59/60\n",
      "40050/40050 [==============================] - 186s 5ms/step - loss: 0.1426 - acc: 0.9418\n",
      "Epoch 60/60\n",
      "40050/40050 [==============================] - 180s 5ms/step - loss: 0.1413 - acc: 0.9432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c32f1ff080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential, Model, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, SpatialDropout1D, Input, GRU, GlobalAveragePooling1D, Conv1D, GlobalMaxPooling1D, LSTM, concatenate, BatchNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model = Sequential()  \n",
    "\n",
    "\n",
    "model.add(Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_shape = Input(shape=(180,))\n",
    "#x = Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False)(input_shape)\n",
    "#x1 = SpatialDropout1D(0.5)(x)\n",
    "\n",
    "#x_gru = Bidirectional(GRU(128, return_sequences=True))(x1)\n",
    "#x1 = Conv1D(32, 4, padding='valid')(x_gru)\n",
    "#avg_pool1_gru  = GlobalAveragePooling1D()(x1)\n",
    "#max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "#x3 = Conv1D(32, 4, padding='valid')(x_gru)\n",
    "#avg_pool3_gru  = GlobalAveragePooling1D()(x3)\n",
    "#max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "#x_lstm = Bidirectional(LSTM(128, return_sequences=True))(x1)\n",
    "#x1 = Conv1D(32, 4, padding='valid')(x_lstm)\n",
    "#avg_pool1_lstm  = GlobalAveragePooling1D()(x1)\n",
    "#max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "#x3 = Conv1D(32, 4, padding='valid')(x_lstm)\n",
    "#avg_pool3_lstm  = GlobalAveragePooling1D()(x3)\n",
    "#max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "#x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru, avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dense(32, activation='relu')(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = Dense(1, activation='sigmoid')(x)\n",
    "#model = Model(inputs=input_shape, outputs=x)\n",
    "\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(ids, target, nb_epoch=60, batch_size=64, callbacks=[ModelCheckpoint('best_weights-old.h5', monitor='acc')], verbose=1)\n",
    "# predict = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('I:/NLP/data_train/train/senmodel-2.h5')\n",
    "\n",
    "model.save_weights('I:/NLP/data_train/train/posmodel-weight-1.h')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('I:/NLP/data_train/train/posmodel_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "#model.save('I:/NLP/data_train/train/quesmodel-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from keras.models import load_model\n",
    "\n",
    "#import sentimential as st\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('I:/NLP/data_train/train/posmodel_architecture.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('I:/NLP/data_train/train/posmodel-weight-1.h')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2668785]]\n"
     ]
    }
   ],
   "source": [
    "f = ['Tôi nghĩ đây không phải là lần đầu tiên đâu ... Vì theo thái độ và cách ứng xử của các nhân viên lễ tân và bên sale bằng cách đưa hết người này đến người kia ra để không qui trách nhiệm cho một ai .']\n",
    "ids = []\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "split = cleanSentences(line).split()\n",
    "\n",
    "for i, word in enumerate(split):\n",
    "    if i >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        ids.append(wordsList.index(word))\n",
    "    except ValueError:\n",
    "        ids.append(0)\n",
    "ids = np.array(ids + ([0] * (maxSeqLength - len(ids))))\n",
    "ids = np.expand_dims(ids, axis=0)\n",
    "prediction = model.predict(ids)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
