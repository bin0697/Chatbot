{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n",
      "Index of `ngon` in wordsList:  1906\n",
      "Vector representation of `ngon` is:  [-1.365e-01 -5.560e-02  1.195e-01  4.790e-02 -8.640e-02 -2.430e-02\n",
      " -3.130e-02 -4.500e-02  4.340e-02 -1.074e-01 -9.460e-02 -5.220e-02\n",
      " -9.380e-02 -4.480e-02 -1.220e-02 -6.000e-03  3.570e-02 -5.500e-02\n",
      " -6.990e-02  6.200e-03  9.800e-03 -4.970e-02  8.430e-02  9.160e-02\n",
      "  1.402e-01 -3.240e-02  3.970e-02 -8.270e-02 -3.770e-02 -7.240e-02\n",
      "  2.710e-02  9.450e-02 -9.180e-02 -1.171e-01  6.750e-02 -8.980e-02\n",
      "  1.854e-01 -1.097e-01 -1.230e-02  2.870e-02 -1.274e-01  5.160e-02\n",
      " -3.800e-03 -6.360e-02  4.900e-03  6.400e-02  1.466e-01 -4.950e-02\n",
      "  9.050e-02 -1.720e-02  1.684e-01  1.850e-02 -6.220e-02 -6.320e-02\n",
      " -9.450e-02 -7.700e-02  1.410e-02 -5.110e-02 -2.300e-03  7.680e-02\n",
      " -5.090e-02 -9.790e-02  2.660e-02  1.153e-01  3.750e-02  1.185e-01\n",
      "  4.650e-02  6.500e-03  1.913e-01  7.500e-02  6.960e-02 -6.130e-02\n",
      " -5.210e-02 -5.870e-02  5.530e-02  4.530e-02  3.770e-02 -9.500e-03\n",
      " -5.860e-02  1.187e-01  8.800e-03 -5.590e-02 -9.490e-02 -1.969e-01\n",
      " -8.430e-02 -1.160e-02  3.590e-02  4.070e-02 -4.020e-02  8.170e-02\n",
      "  7.110e-02  3.170e-02 -1.597e-01 -5.290e-02 -7.000e-03  1.852e-01\n",
      " -1.020e-02  7.180e-02  1.429e-01  7.100e-03  8.690e-02  7.630e-02\n",
      "  7.810e-02 -1.732e-01  3.790e-02 -1.025e-01 -1.390e-02  1.253e-01\n",
      " -1.940e-02 -1.036e-01 -1.684e-01 -1.010e-02 -5.920e-02 -8.340e-02\n",
      "  3.800e-03 -2.300e-02  2.810e-02 -2.660e-02  1.052e-01 -8.460e-02\n",
      " -7.120e-02 -4.930e-02 -7.900e-03  2.490e-02  5.660e-02  5.570e-02\n",
      "  4.860e-02 -7.550e-02  1.048e-01  2.610e-02  3.650e-02 -1.352e-01\n",
      " -1.938e-01 -5.030e-02  1.770e-01 -4.720e-02  1.247e-01  9.930e-02\n",
      "  1.320e-02 -2.930e-02 -1.305e-01 -3.410e-02  5.740e-02 -9.250e-02\n",
      "  8.210e-02  6.650e-02  6.980e-02 -4.090e-02 -1.000e-04 -5.500e-02\n",
      " -7.460e-02 -7.730e-02  2.990e-02 -5.230e-02  4.180e-02  5.340e-02\n",
      "  2.560e-02 -1.060e-02  1.400e-02  3.930e-02  6.760e-02 -3.070e-02\n",
      " -5.260e-02 -1.300e-03 -1.600e-02  2.552e-01 -4.840e-02 -8.460e-02\n",
      " -1.333e-01 -1.300e-03  1.710e-02 -8.240e-02 -1.800e-03 -6.720e-02\n",
      " -1.223e-01  6.800e-03  2.680e-02  1.084e-01 -5.000e-03  1.085e-01\n",
      " -9.700e-02 -2.880e-02  2.079e-01 -1.180e-02  2.071e-01 -5.390e-02\n",
      " -3.300e-03  6.610e-02  8.220e-02 -6.190e-02  1.084e-01  3.550e-02\n",
      "  6.270e-02  2.430e-02  7.380e-02  2.826e-01  5.850e-02  7.500e-03\n",
      "  2.774e-01 -1.166e-01 -7.270e-02 -3.140e-02 -2.740e-02 -4.410e-02\n",
      "  7.490e-02 -1.020e-02 -1.530e-01  1.706e-01  2.160e-02 -1.091e-01\n",
      " -3.840e-02 -3.070e-02  5.800e-03 -2.520e-02  4.000e-02  2.170e-02\n",
      "  1.001e-01  2.810e-02 -9.630e-02 -1.270e-02  5.590e-02 -1.352e-01\n",
      "  7.750e-02  8.470e-02  3.170e-02 -3.100e-02  9.720e-02 -1.330e-01\n",
      "  4.000e-04 -2.455e-01  1.037e-01  2.300e-02 -2.200e-03 -1.330e-01\n",
      "  1.040e-02 -4.400e-02 -2.900e-03  3.800e-03 -1.030e-01 -1.687e-01\n",
      "  8.990e-02 -2.840e-02 -3.410e-02  4.570e-02 -1.460e-02  5.350e-02\n",
      " -9.700e-02  1.170e-02 -7.730e-02 -1.870e-02  1.330e-01  3.870e-02\n",
      "  1.425e-01 -3.190e-02  1.725e-01  1.480e-02 -2.320e-02  1.930e-01\n",
      "  1.257e-01 -5.900e-02  6.250e-02  6.700e-03 -1.404e-01 -2.820e-02\n",
      "  2.780e-02  3.090e-02 -7.000e-03 -2.660e-02  3.820e-02  1.320e-02\n",
      " -1.056e-01 -3.640e-02 -5.580e-02  3.890e-02 -2.080e-02 -3.670e-02\n",
      " -4.250e-02 -1.636e-01 -3.290e-02 -4.550e-02  2.970e-02  1.287e-01\n",
      "  1.393e-01 -1.080e-01 -4.070e-02  5.250e-02 -1.031e-01 -1.060e-02\n",
      "  6.090e-02  8.550e-02  3.520e-02 -1.066e-01 -1.100e-02  1.051e-01\n",
      "  6.740e-02 -3.930e-02 -9.590e-02 -1.300e-03 -1.234e-01  2.150e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "currentDir = 'I:/NLP/data_train/'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "#wordsList = np.load(f'{currentDir}wordsList.npy')\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "#wordVectors = np.load(f'{currentDir}wordVectors.npy')\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')\n",
    "ngon_idx = wordsList.index('bán')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9448\n",
      "[  10181  314185  314185   10181   11505   11066  350794   10065   10157\n",
      "   10059   10194   10013  314185   10213   10213   10213 1264310   10005\n",
      "   10011   10013   10019   10038  275576   10210   10210   10030   10027\n",
      "   10744   10065  604258   10011   10011   10023   10545   10196   10400\n",
      "   10521   10637  368564   10011  811496   10023   10023   15980   13438\n",
      "   12983   18975  140453  611633   10062   10010  164768  701607   10017\n",
      "   13364   10005   12916   10028   10793   10101   10393       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#embed_dim = 128\n",
    "#lstm_out = 196\n",
    "x =  np.load(f'I:/NLP/data_train/idsMatrixques-2.npy')\n",
    "print (len(x))\n",
    "#X = open('I:/NLP/data_train/idsMatrixques.npy', \"r\", encoding='utf-8')\n",
    "print (x[2999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range (1,1531):\n",
    "    data.append(x[i])\n",
    "    target.append(1)\n",
    "\n",
    "for i in range (3831,7553):\n",
    "    data.append(x[i])\n",
    "    target.append(0)\n",
    "    \n",
    "for i in range (1531,1831):\n",
    "    x_test.append(x[i])\n",
    "    y_test.append(1)\n",
    "for i in range (1831,3831):\n",
    "    x_test.append(x[i])\n",
    "    y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10247  10452  10110 122254  10005  10132  10149  94194 263461  10055\n",
      "  10047 374498  11029  10057  10607  39066  10092   9999  10405  10076\n",
      "  10040      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (data[1])\n",
    "print (target[1831])\n",
    "# target = np.array(target)\n",
    "# target = target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THẠNH\n",
      "[ 2.22999994e-02  2.69000009e-02  1.89999994e-02  5.29999994e-02\n",
      "  1.59000009e-02 -3.62999998e-02 -2.04000007e-02 -8.99999985e-04\n",
      "  9.60000046e-03 -3.92999984e-02  9.99999978e-03 -1.02000004e-02\n",
      "  2.08999999e-02  1.82000007e-02 -1.78999994e-02 -2.06000004e-02\n",
      " -2.91000009e-02 -1.71000008e-02 -1.95000004e-02 -1.47000002e-02\n",
      " -2.05000006e-02  4.45000008e-02 -2.08999999e-02  1.44999996e-02\n",
      " -1.41000003e-02  5.49999997e-03 -4.49999981e-03 -9.49999969e-03\n",
      "  3.80000006e-03 -1.26000000e-02 -1.20999999e-02  1.00000005e-03\n",
      "  2.82000005e-02 -6.23000003e-02 -1.26999998e-02 -2.89999996e-03\n",
      " -2.34999992e-02  1.52000003e-02  1.85000002e-02 -2.07000002e-02\n",
      "  2.49999994e-03  2.84000002e-02  1.40300006e-01 -3.68999988e-02\n",
      "  1.35000004e-02 -4.49000001e-02 -9.70000029e-03  8.00000038e-03\n",
      " -3.00000003e-03  2.87999995e-02 -9.70000029e-03 -2.60000001e-03\n",
      " -5.35999984e-02  1.78999994e-02  6.99999975e-04 -9.53999981e-02\n",
      " -3.50000001e-02 -3.04000005e-02  1.15000000e-02 -7.60000013e-03\n",
      " -4.10000002e-03 -2.17000004e-02  6.89999992e-03 -1.89999994e-02\n",
      "  1.99999995e-04  1.00000005e-03 -1.99999995e-04  5.09999990e-02\n",
      "  1.31999999e-02 -2.32999995e-02  1.76999997e-02  3.73999998e-02\n",
      " -3.19999992e-03 -2.97999997e-02 -3.29000019e-02 -6.69999979e-03\n",
      "  4.90000006e-03  2.16000006e-02 -9.80000012e-03 -3.50000011e-03\n",
      "  8.60000029e-03 -2.96999998e-02  2.25000009e-02  8.99999961e-03\n",
      " -9.60000046e-03  1.60000008e-02  2.08999999e-02  2.74999999e-02\n",
      "  4.19999985e-03  2.83000004e-02 -8.29999987e-03 -3.03000007e-02\n",
      "  1.48000000e-02  2.07000002e-02  4.32000011e-02 -5.11000007e-02\n",
      "  2.11999994e-02 -1.31999999e-02  6.59999996e-03  7.00000022e-03\n",
      " -9.20000020e-03 -2.20999997e-02 -2.30000005e-03 -2.09999993e-03\n",
      " -1.96000002e-02  2.67999992e-02  9.89999995e-03 -2.96000000e-02\n",
      " -3.48999985e-02  2.74000000e-02  7.69999996e-03  5.10000018e-03\n",
      " -1.09999999e-03  1.00999996e-02 -1.33999996e-02 -1.62000004e-02\n",
      " -1.76999997e-02  1.72000006e-02  3.00000003e-03  8.10000021e-03\n",
      "  2.44999994e-02 -2.59000007e-02 -5.00000007e-02 -3.20000015e-02\n",
      " -1.49999997e-02 -1.76999997e-02 -0.00000000e+00 -9.99999975e-05\n",
      " -2.08000001e-02 -3.57999988e-02  1.75000001e-02 -4.30000015e-03\n",
      " -1.00000005e-03 -1.70000009e-02 -2.51000002e-02 -9.30000003e-03\n",
      " -2.04000007e-02  4.99999989e-03  2.19999999e-03  3.10000009e-03\n",
      " -1.30000003e-02  4.05000001e-02 -9.30000003e-03  8.20000004e-03\n",
      " -4.65000011e-02 -2.85000000e-02  5.70000010e-03 -4.01000008e-02\n",
      " -3.29999998e-03  1.83000006e-02  0.00000000e+00  4.80000023e-03\n",
      "  3.51999998e-02 -7.60000013e-03  2.15000007e-02 -2.42999997e-02\n",
      "  3.99999990e-04 -3.89999989e-03 -1.74000002e-02  1.00000005e-03\n",
      " -1.20999999e-02 -2.30000005e-03  3.02000009e-02 -1.09000001e-02\n",
      " -8.00000038e-03  2.26500005e-01 -4.94000018e-02  1.08000003e-02\n",
      "  5.31999990e-02  4.63000014e-02  2.00999994e-02 -3.61000001e-02\n",
      " -1.31999999e-02 -1.53000001e-02 -3.86999995e-02  2.47000009e-02\n",
      " -2.44999994e-02  1.16999997e-02 -1.93000007e-02 -1.79999992e-02\n",
      " -2.78999992e-02 -1.65999997e-02  7.69999996e-03  1.30000000e-03\n",
      " -3.00000014e-04 -1.14000002e-02 -2.97999997e-02  1.35000004e-02\n",
      "  7.19999988e-03 -6.06999993e-02  1.70000002e-03  9.12000015e-02\n",
      " -1.20000001e-02 -9.80000012e-03 -2.55999994e-02  1.09999999e-03\n",
      "  4.00000019e-03  3.00000014e-04 -3.17000002e-02  4.78999987e-02\n",
      "  3.06000002e-02 -4.30000015e-03 -2.19999999e-03  1.29000004e-02\n",
      " -4.90000006e-03  1.63000003e-02  4.49999981e-03  2.00000009e-03\n",
      " -1.00999996e-02 -3.99999991e-02 -2.47000009e-02 -1.66999996e-02\n",
      "  2.41000000e-02  1.16999997e-02  2.38000005e-02  1.27999997e-02\n",
      " -3.59999994e-03  3.15000005e-02  1.21999998e-02 -2.30000000e-02\n",
      " -4.69999993e-03 -9.60000046e-03  6.30000001e-03 -1.19000003e-02\n",
      " -5.00000024e-04  1.79999992e-02 -1.20000006e-03  1.56999994e-02\n",
      "  2.01999992e-02 -6.20000018e-03 -1.39999995e-03  7.60000013e-03\n",
      " -4.80000023e-03 -3.83000001e-02  1.64999999e-02 -2.89999996e-03\n",
      " -1.26000000e-02 -4.49999981e-03  2.56999992e-02  2.34999992e-02\n",
      "  4.00000019e-03  1.79999992e-02  3.99000011e-02  1.53000001e-02\n",
      "  1.29000004e-02  1.73000004e-02  1.19000003e-02 -7.60000013e-03\n",
      " -1.55999996e-02  2.86999997e-02  1.71000008e-02 -1.42999999e-02\n",
      "  2.96000000e-02 -5.40000014e-03 -1.31999999e-02 -9.39999986e-03\n",
      " -2.40000011e-03 -4.00000019e-03  1.56999994e-02  1.51000004e-02\n",
      "  1.22999996e-02  4.03999984e-02 -2.41999999e-02 -1.93000007e-02\n",
      " -4.39999998e-03  4.06000018e-02 -1.37999998e-02  3.40999998e-02\n",
      " -5.20000001e-03  3.97000015e-02 -3.80000006e-03 -3.99999990e-04\n",
      " -3.59999985e-02 -1.31999999e-02  2.30000005e-03 -4.94999997e-02\n",
      " -1.66999996e-02  1.59000009e-02 -3.29999998e-03  3.09999995e-02\n",
      "  1.36000002e-02 -1.46000003e-02  2.20999997e-02  2.67999992e-02\n",
      " -1.70000002e-03  2.17000004e-02  7.30000017e-03  2.20999997e-02\n",
      " -2.40000011e-03  4.52000014e-02 -2.63999999e-02 -1.59000009e-02\n",
      "  1.25000002e-02  8.50000046e-03 -5.20999990e-02  2.32999995e-02\n",
      " -2.19999999e-02 -3.97000015e-02 -1.26000000e-02  1.21999998e-02]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "t_train = h5py.File('I:/NLP/data_train/cc.vi.300.hdf5')\n",
    "print(t_train['words'][100010])\n",
    "t_train['vectors'][0].shape\n",
    "print(t_train['vectors'][99990])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "wordsList = t_train['words']\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "#wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "# wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = t_train['vectors']\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 300)          600000000 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 600,219,777\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 600,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5252 samples, validate on 2300 samples\n",
      "Epoch 1/20\n",
      "5252/5252 [==============================] - 64s 12ms/step - loss: 0.6119 - acc: 0.7075 - val_loss: 0.5167 - val_acc: 0.8696\n",
      "Epoch 2/20\n",
      "5252/5252 [==============================] - 51s 10ms/step - loss: 0.6032 - acc: 0.7087 - val_loss: 0.4584 - val_acc: 0.8696\n",
      "Epoch 3/20\n",
      "5252/5252 [==============================] - 52s 10ms/step - loss: 0.6002 - acc: 0.7087 - val_loss: 0.4481 - val_acc: 0.8696\n",
      "Epoch 4/20\n",
      "5252/5252 [==============================] - 49s 9ms/step - loss: 0.5249 - acc: 0.7584 - val_loss: 0.3799 - val_acc: 0.8687\n",
      "Epoch 5/20\n",
      "5252/5252 [==============================] - 55s 10ms/step - loss: 0.4110 - acc: 0.8524 - val_loss: 0.4998 - val_acc: 0.8730\n",
      "Epoch 6/20\n",
      "5252/5252 [==============================] - 57s 11ms/step - loss: 0.3697 - acc: 0.8726 - val_loss: 0.4628 - val_acc: 0.8761\n",
      "Epoch 7/20\n",
      "5252/5252 [==============================] - 50s 9ms/step - loss: 0.3156 - acc: 0.8875 - val_loss: 0.4660 - val_acc: 0.8578\n",
      "Epoch 8/20\n",
      "5252/5252 [==============================] - 49s 9ms/step - loss: 0.1455 - acc: 0.9488 - val_loss: 0.4984 - val_acc: 0.8313\n",
      "Epoch 9/20\n",
      "5252/5252 [==============================] - 49s 9ms/step - loss: 0.0707 - acc: 0.9810 - val_loss: 0.6499 - val_acc: 0.7926\n",
      "Epoch 10/20\n",
      "5252/5252 [==============================] - 49s 9ms/step - loss: 0.0473 - acc: 0.9872 - val_loss: 0.5908 - val_acc: 0.8043\n",
      "Epoch 11/20\n",
      "5252/5252 [==============================] - 49s 9ms/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.6343 - val_acc: 0.8404\n",
      "Epoch 12/20\n",
      "5252/5252 [==============================] - 51s 10ms/step - loss: 0.0247 - acc: 0.9937 - val_loss: 1.0007 - val_acc: 0.7826\n",
      "Epoch 13/20\n",
      "5252/5252 [==============================] - 51s 10ms/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.7338 - val_acc: 0.8387\n",
      "Epoch 14/20\n",
      "5252/5252 [==============================] - 51s 10ms/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.5560 - val_acc: 0.8770\n",
      "Epoch 15/20\n",
      "5252/5252 [==============================] - 50s 10ms/step - loss: 0.0155 - acc: 0.9962 - val_loss: 0.9445 - val_acc: 0.8248\n",
      "Epoch 16/20\n",
      "5252/5252 [==============================] - 50s 10ms/step - loss: 0.0128 - acc: 0.9964 - val_loss: 1.0591 - val_acc: 0.8300\n",
      "Epoch 17/20\n",
      "5252/5252 [==============================] - 50s 9ms/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.7316 - val_acc: 0.8635\n",
      "Epoch 18/20\n",
      "5252/5252 [==============================] - 50s 10ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.4611 - val_acc: 0.9217\n",
      "Epoch 19/20\n",
      "5252/5252 [==============================] - 50s 9ms/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.4699 - val_acc: 0.9183\n",
      "Epoch 20/20\n",
      "5252/5252 [==============================] - 51s 10ms/step - loss: 0.0125 - acc: 0.9964 - val_loss: 0.9368 - val_acc: 0.8213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258a04b0518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()  \n",
    "# model.add(LSTM(100, input_shape=(1, 100),return_sequences=True))\n",
    "# model.add(Dense(100))\n",
    "# model.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n",
    "# model.fit(data, target, nb_epoch=10, batch_size=20, verbose=2,validation_data=(x_test, y_test))\n",
    "\n",
    "max_fatures = 2000\n",
    "embed_dim = 128\n",
    "lstm_out = 196 \n",
    "# model.add(Embedding(max_fatures, embed_dim,input_length = 185))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# model.fit(data, target, nb_epoch=10, batch_size=20, verbose=2,validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "model.add(Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#model.add(Embedding(len(wordVectors), 300, weights=[wordVectors], input_length=180, trainable=False))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(np.array(data), np.array(target), nb_epoch=20, batch_size=34, verbose=1 , validation_data=(np.array(x_test), np.array(y_test) ))\n",
    "# predict = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('I:/NLP/data_train/train/senmodel-2.h5')\n",
    "\n",
    "model.save_weights('I:/NLP/data_train/train/quesmodel-weight-final(1).h')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('I:/NLP/data_train/train/quessmodel_architecture-final(1).json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "#model.save('I:/NLP/data_train/train/quesmodel-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/cc.vi.300.vec', encoding = \"utf8\") as fp:\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10065   10071   10157 1495622   10006       0   10118   10047   11540\n",
      "    12776  796791   10064       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0]]\n",
      "[[0.00338959]]\n",
      "cmt\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import re\n",
    "f = ['Mình chỉ muốn góp ý là nhà vệ sinh cần phải đc dọn sạch sẽ hơn']\n",
    "ids = []\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "nline =  word_tokenize(cleanSentences(line), format=\"text\")\n",
    "newline = re.sub(r'_', \"\", nline)\n",
    "split = newline.split()\n",
    "\n",
    "for i, word in enumerate(split):\n",
    "    if i >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        idx = 0\n",
    "        for x in range(2000000):\n",
    "            #line = fp.readline().strip()\n",
    "            doline = lines [x] \n",
    "            #while line:\n",
    "            z = doline.split(' ', 1)[0]\n",
    "            #if x == 200000:\n",
    "            if z == word:\n",
    "                idx = x + 9998\n",
    "                break\n",
    "        #idx = wordsList.index(split[word_idx])\n",
    "        ids.append(idx)\n",
    "        #ids.append(wordsList.index(word))\n",
    "    except ValueError:\n",
    "        ids.append(518835)\n",
    "        \n",
    "ids = np.array(ids + ([0] * (maxSeqLength - len(ids))))\n",
    "ids = np.expand_dims(ids, axis=0)\n",
    "prediction = model.predict(ids)\n",
    "print (ids)\n",
    "print(prediction)\n",
    "if prediction < 0.5: \n",
    "    print ('cmt')\n",
    "else: \n",
    "    print ('ques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THẠNH\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "filepath = 'I:/NLP/data_train/tu.npy'\n",
    "tuw = []\n",
    "t_train = h5py.File('I:/NLP/data_train/cc.vi.300.hdf5')\n",
    "print(t_train['words'][100010])\n",
    "#print(t_train['words'][10000:10010])\n",
    "#print(t_train['vectors'][99990])\n",
    "\n",
    "# for x in range(10000,2000000):\n",
    "#             line = t_train['words'][x]\n",
    "#             #while line:\n",
    "#             #z = line.split(' ', 1)[0]\n",
    "#             #if x == 200000:\n",
    "#             if line == 'UNK':\n",
    "#                 print ('index is: ' + str(x))\n",
    "#                 print (line)\n",
    "#                 #print (line.split(' ', 1)[1])\n",
    "#                 break\n",
    "                \n",
    "with open(filepath, 'wb') as tu:\n",
    "    for x in t_train['words']:\n",
    "        #str(x)\n",
    "        y = x.encode('utf8')\n",
    "        tuw.append(y)\n",
    "        #print (tuw)\n",
    "#         y = str(x) \n",
    "#         print 󰀀\n",
    "    np.save(tu, tuw)\n",
    "        #tu.append(x)\n",
    "            \n",
    "tu.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mình\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "filepath = 'I:/NLP/data_train/tu.npy'\n",
    "wordslist = np.load(filepath)\n",
    "wordsList = [word.decode('UTF-8') for word in wordslist]\n",
    "print (wordsList[10065])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10065   10071   10157 1495622   10006  518835   10118   10047   11540\n",
      "    12776  796791   10064       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0       0       0       0]]\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import re\n",
    "f = ['Mình chỉ muốn góp ý là nhà vệ sinh cần phải đc dọn sạch sẽ hơn']\n",
    "ids = []\n",
    "maxSeqLength = 180\n",
    "line = f[0]\n",
    "nline =  word_tokenize(cleanSentences(line), format=\"text\")\n",
    "newline = re.sub(r'_', \"\", nline)\n",
    "split = newline.split()\n",
    "for i, word in enumerate(split):\n",
    "    if i >= maxSeqLength:\n",
    "        break\n",
    "    try:\n",
    "        idx = wordsList.index(word)\n",
    "        ids.append(idx)\n",
    "#         for x in range(100000,2000000):\n",
    "#             #line = fp.readline().strip()\n",
    "#             doline = wordsList[x] \n",
    "#             #while line:\n",
    "#             #z = doline.split(' ', 1)[0]\n",
    "#             #if x == 200000:\n",
    "#             if doline == word:\n",
    "#                 print(doline)\n",
    "#                 idx = x\n",
    "#                 break\n",
    "#         #idx = wordsList.index(split[word_idx])\n",
    "#         ids.append(idx)\n",
    "#         #ids.append(wordsList.index(word))\n",
    "    except ValueError:\n",
    "        ids.append(518835)\n",
    "        \n",
    "ids = np.array(ids + ([0] * (maxSeqLength - len(ids))))\n",
    "ids = np.expand_dims(ids, axis=0)\n",
    "print (ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
