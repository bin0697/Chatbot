{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo máy ảo bằng Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill kernel cũ đi đồng thời tạo kết nối mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện cần thiết và chứng thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện liên quan đến việc kết nối Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân tích cảm xúc với LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng ta sẽ dùng mạng LSTM để giải quyết bài toán phân tích cảm xúc (Sentiment Analysis) trên tập dữ liệu văn bản. Nếu nhìn theo kiểu black box, đầu vào của bài toán là một câu hoặc đoạn văn bản và đầu ra là trạng thái tích cực, tiêu cực hay trung hoà (positive - negative - neutral). Trong phạm vi của assignment này, chúng ta chỉ quan tâm đến hai trạng thái cảm xúc là positive và negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Góc nhìn Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chúng ta giữ nguyên định dạng đầu vào là chuỗi ký tự thì rất khó để thực hiện các thao tác biến đổi như tích vô hướng (dot product) hoặc các thuật toán trên mạng neural network như backpropagation. Thay vì dữ liệu đầu vào là một chuỗi, chúng ta cần chuyển đổi các từ trong tập từ điển sang dạng vector số học trong đó có thể thực hiện được các phép toán nêu trên.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong hình minh hoạ ở trên, ta có thể hình dung dữ liệu đầu vào của thuật toán phân tích cảm xúc là một ma trận 16 x D chiều. Trong đó 16 là số lượng từ trong câu và D là số chiều của không gian vector để biểu diễn từ. Để ánh xạ từ một từ sang một vector, chúng ta sử dụng ma trận word embedding như đã thực hiện trong bài Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng tôi sử dụng tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn. Trong đó có 15,000 mẫu positive và 15,000 mẫu negative. Nguồn: https://streetcodevn.com/blog/dataset. Tập dữ liệu này đã được đính kèm trong thư mục của assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các bước để huấn luyện trên mạng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 5 bước chính để giải quyết bài toán phân tích cảm xúc trong văn bản:\n",
    "\n",
    "    1) Huấn luyện một mô hình phát sinh ra vector từ (như mô hình Word2Vec) hoặc tải lên các vector từ tiền huấn luyện.\n",
    "    2) Tạo ma trận ID cho tập dữ liệu huấn luyện\n",
    "    3) Tạo mô hình RNN với các đơn vị LSTM, sử dụng tensorflow\n",
    "    4) Huấn luyện mô hình RNN với dữ liệu ma trận đã tạo ở bước 2\n",
    "    5) Đánh giá mô hình đã huấn luyện với tập test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load tập từ vựng và ma trận word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, để có thể biến đổi một từ thành một vector, chúng ta sử dụng mô hình đã được huấn luyện trước đó (pretrained model). Mô hình đã train trước đó cho tiếng Việt được lấy ở đây: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhiên, số lượng từ vựng tiếng Việt được huấn luyện rất lớn, khoảng 2M từ. Mỗi từ được biểu diễn dưới dạng một vector 300 chiều. Với kích thước gốc của ma trận word embedding như vậy sẽ gây khó khăn cho việc load dữ liệu cũng như đưa vào thư viện tensorflow để huấn luyện nên chúng tôi đã tối giản lại với số lượng từ tối thiểu để có thể chạy được trên tập dữ liệu review về đồ ăn của Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# LƯU Ý: CẦN PHẢI CHỈNH LẠI ĐƯỜNG DẪN NÀY THÀNH THƯ MỤC CHỨA CÁC FILE ASSIGNMENT3\n",
    "# CHỮ 'drive' có nghĩa là thư mục mặc định của Google drive\n",
    "currentDir = 'I:/NLP/data_train/'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "#wordsList = np.load(f'{currentDir}wordsList.npy')\n",
    "\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "#wordVectors = np.load(f'{currentDir}wordVectors.npy')\n",
    "\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chắc chắn mọi dữ liệu được load lên một cách chính xác, chúng ta cần kiểm tra xem số lượng từ trong từ điển rút gọn và số chiều của ma trận word embedding có khớp với nhau hay không? Trong trường hợp này số từ mà chúng tôi giữ lại là 19,899 và số chiều trong không gian biểu diễn là 300 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trên một từ đơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có thể xác định được vector biểu diễn của một từ tiếng Việt. Đầu tiên chúng ta sẽ xác định xem vị trí của từ đó trong wordsList. Sau đó lấy vector ở dòng tương ứng trên trên ma trận wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  11067\n",
      "Vector representation of `ngon` is:  [ 0.0594 -0.0387  0.0636  0.007  -0.0432 -0.035   0.0447  0.055   0.0171\n",
      "  0.0166  0.0408 -0.0175 -0.0572 -0.0414 -0.0142  0.0234  0.0542 -0.0311\n",
      " -0.0516 -0.0417  0.0616 -0.0209  0.0256 -0.0175  0.0343  0.03    0.0514\n",
      " -0.016   0.0489  0.0246 -0.0005  0.0743 -0.0088 -0.0592 -0.0332  0.0113\n",
      "  0.0282 -0.0607 -0.0059  0.0463  0.0294  0.0155 -0.0011 -0.0037  0.0003\n",
      "  0.0363 -0.0252 -0.0275 -0.0565 -0.0219 -0.0248  0.0003 -0.05   -0.0128\n",
      " -0.0227  0.0314 -0.0361 -0.0054 -0.018  -0.01   -0.0246 -0.0156 -0.0045\n",
      " -0.0786 -0.018  -0.0128 -0.0012  0.0038  0.1431 -0.0118 -0.0054 -0.0038\n",
      "  0.0192  0.0062 -0.0008 -0.0282 -0.0271 -0.0486  0.0445  0.0566 -0.0128\n",
      " -0.0255 -0.0101  0.0097  0.0006  0.036   0.0099  0.0062  0.0027  0.0339\n",
      "  0.0018 -0.0827  0.017   0.0265 -0.003  -0.0685  0.0177  0.0099  0.0362\n",
      "  0.0268 -0.131  -0.0347  0.0285  0.0447 -0.1033 -0.0504 -0.044   0.053\n",
      " -0.0187  0.0172 -0.0789 -0.0102  0.0275  0.0426 -0.0166  0.0148 -0.0185\n",
      "  0.0206  0.0292 -0.0645  0.0004 -0.0128  0.0081  0.0031  0.014  -0.0546\n",
      "  0.0134 -0.0886 -0.0192  0.0155  0.0106 -0.0221 -0.0168 -0.053  -0.0084\n",
      " -0.0536  0.0549  0.0357  0.0373  0.0005 -0.0165 -0.063  -0.0555  0.0012\n",
      " -0.0157 -0.0297 -0.0301  0.0677 -0.0309 -0.0485 -0.0287  0.0561  0.0089\n",
      "  0.0068 -0.0006  0.0067 -0.0042  0.0349 -0.0146  0.0182 -0.013  -0.0259\n",
      " -0.0362  0.0099 -0.0347  0.0885  0.0309  0.0179  0.0084 -0.0061 -0.0442\n",
      " -0.0207 -0.0369  0.0003  0.0103 -0.0429  0.0344  0.093   0.0152 -0.0188\n",
      " -0.0185 -0.0429  0.0742 -0.0249  0.1     0.0179  0.0994  0.0003 -0.0408\n",
      "  0.0052 -0.0429 -0.0182 -0.0311 -0.0251  0.0059  0.0114 -0.0401  0.0057\n",
      "  0.0165 -0.0187  0.0099 -0.0082 -0.0087 -0.0571 -0.0332  0.0447 -0.0188\n",
      "  0.0067 -0.001  -0.0215  0.1349 -0.0425 -0.0175 -0.0005  0.0755  0.0095\n",
      "  0.0449  0.0192 -0.0352 -0.0034 -0.0247 -0.0407  0.0057 -0.0158 -0.0852\n",
      " -0.0215  0.0034  0.0109  0.0288 -0.0857 -0.0007 -0.0316 -0.0114  0.0864\n",
      " -0.0792  0.0162  0.017  -0.0381 -0.0072  0.0305  0.0774 -0.0143 -0.0599\n",
      " -0.0563  0.0242 -0.0166 -0.0237  0.0256  0.0964 -0.0085 -0.028  -0.026\n",
      "  0.0861  0.0462  0.1793  0.0076 -0.0352  0.0065 -0.0266  0.0282 -0.0008\n",
      " -0.0344 -0.0678 -0.0135 -0.0357 -0.0013 -0.0385 -0.0114 -0.0218  0.0244\n",
      "  0.0278 -0.0462  0.0057 -0.0325  0.0787 -0.0429 -0.0513  0.0109  0.0154\n",
      "  0.0492  0.0063  0.0376 -0.0149 -0.0083 -0.0384  0.0241 -0.0051  0.0013\n",
      " -0.0414  0.0018 -0.0094  0.0384  0.0352  0.0162 -0.0264  0.0309 -0.0116\n",
      " -0.1515 -0.0216  0.0167]\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('tối_cao')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec để biểu diễn một đoạn văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nâng cấp hơn so với phiên bản Word2Vec cho từ đơn, phần này chúng ta sẽ biểu diễn một câu dưới dạng một ma trận gồm các vector biểu diễn của từng từ chồng lên nhau.\n",
    "\n",
    "Ví dụ như chúng ta muốn biểu diễn câu \"Món này ăn hoài không biết chán\". Đầu tiên, với mỗi từ trong câu ta sẽ tìm chỉ số tương ứng trong từ điển và lưu vào vector đặt tên là 'sentenceIndexes'. Sau đó, chúng ta có thể sử dụng hàm tra cứu ma trận word embedding của thư viện Tensorflow tf.nn.embedding_lookup để tra các vector tại các chỉ số trong 'sentenceIndexes'. Như vậy nếu chúng ta sử dụng tối đa 10 từ để lưu trữ cho một câu thì ma trận biểu diễn cho câu sẽ là một ma trận kích thước 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sentence = 'Món này ăn hoài không biết chán'\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
    "word_in_sentence = [word.lower() for word in sentence.split()]\n",
    "for i in range(len(word_in_sentence)):\n",
    "    idx =  wordsList.index(word_in_sentence[i])\n",
    "    sentenceIndexes[i] = idx\n",
    "    \n",
    "# Các chỉ số 7, 8, 9 của sentenceIndexes  vẫn được gán bằng 0 như cũ\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma trận biểu diễn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như thực hiện đúng thì vector 'sentenceIndexes' sẽ có giá trị là: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Khảo sát tập dữ liệu huấn luyện và tạo ma trận ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, chúng tôi sử dụng tập dữ liệu lấy từ trang web Foody trên miền dữ liệu liên quan đến ẩm thực. Tập dữ liệu bao gôm 15.000 review tích cực đặt trong thư mục 'positiveReviews' và 15.000 review tiêu cực đặt trong thư mục 'negativeReviews'. Do khối lượng dữ liệu lớn, nếu chúng ta chọn số lượng từ tối đa (maxSeqLength) quá cao thì sẽ bị lãng phí khi biểu diễn ở những câu review quá ngắn. Ngược lại, nếu sử dụng số lượng từ tối đa quá ít thì sẽ bị bỏ lỡ những từ quan trọng giúp cho việc phân tích cảm xúc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 40050\n",
      "The total number of words in the files is 2375531\n",
      "The average number of words in the files is 59.31413233458177\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['I:/NLP/data_train/train/pos/' + f for f in listdir('I:/NLP/data_train/train/pos/') if isfile(join('I:/NLP/data_train/train/pos/', f))]\n",
    "negativeFiles = ['I:/NLP/data_train/train/neg/' + f for f in listdir('I:/NLP/data_train/train/neg/') if isfile(join('I:/NLP/data_train/train/neg/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể sử dụng thư viện Matplot để minh hoạ phân bố về chiều dài của các câu review trong tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHMRJREFUeJzt3X+UHWWd5/H3x0R+I0k0sNkkTMLai6KjIbQhiOOowRCCQ3AG1ng8Sw9mJrO7zKrj7o5B3YmgnIVdVxx2FIkSDawCAUWywExoAzhnZ+VHIhh+T1pAaJMhjQkBRYNhvvtHfS9UQv+4nVR19718Xufcc6u+9VTd56E690s99dynFBGYmZlV6TWjXQEzM2s/Ti5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpWrNblI+gtJD0i6X9JVkg6QNFPSnZI2SbpG0n5Zdv9c78ntM0rHOTfjj0g6uc46m5nZvqstuUiaCnwM6IyItwLjgMXARcDFEdEBbAeW5C5LgO0R8Ubg4iyHpGNyv7cAC4CvShpXV73NzGzf1d0tNh44UNJ44CBgC/A+4Lrcvgo4PZcX5Tq5fZ4kZfzqiNgZEY8BPcCcmuttZmb7YHxdB46In0v6IvAE8GvgFmAD8ExE7MpivcDUXJ4KPJn77pK0A3h9xu8oHbq8z0skLQWWAhx88MHH/fbQKU3X9XenHtZ8w8zM2tSGDRuejojJVRyrtuQiaSLFVcdM4BngWuCUfoo25p/RANsGiu8eiFgBrADo7OyMp086r+m6rr/w1KbLmpm1K0k/q+pYdXaLnQQ8FhF9EfFb4HvAO4EJ2U0GMA3YnMu9wHSA3H4YsK0c72cfMzMbg+pMLk8AcyUdlPdO5gEPArcBZ2SZLuCGXF6T6+T2W6OYVXMNsDhHk80EOoC7aqy3mZntozrvudwp6Trgx8Au4B6KbqubgKslfSFjl+culwNXSuqhuGJZnMd5QNJqisS0CzgnIl6sq95mZrbvaksuABGxHFi+R/hR+hntFRG/Ac4c4DgXABdUXkEzM6uFf6FvZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVrlaZ0VuFTOW3dRv/HE/odLMbK/4ysXMzCrn5GJmZpVzcjEzs8o5uZiZWeVqSy6SjpZ0b+n1rKRPSJokqVvSpnyfmOUl6RJJPZI2SppdOlZXlt8kqauuOpuZWTVqSy4R8UhEzIqIWcBxwPPA9cAyYF1EdADrch3gFKAjX0uBSwEkTQKWA8cDc4DljYRkZmZj00h1i80DfhoRPwMWAasyvgo4PZcXAVdE4Q5ggqQpwMlAd0Rsi4jtQDewYITqbWZme2Gkksti4KpcPiIitgDk++EZnwo8WdqnN2MDxc3MbIyqPblI2g84Dbh2qKL9xGKQ+J6fs1TSeknr+/r6hl9RMzOrzEhcuZwC/Dginsr1p7K7i3zfmvFeYHppv2nA5kHiu4mIFRHRGRGdkydPrrgJZmY2HCORXD7My11iAGuAxoivLuCGUvysHDU2F9iR3WZrgfmSJuaN/PkZMzOzMarWucUkHQS8H/izUvhCYLWkJcATwJkZvxlYCPRQjCw7GyAitkn6PHB3ljs/IrbVWW8zM9s3tSaXiHgeeP0esV9QjB7bs2wA5wxwnJXAyjrqaGZm1fMv9M3MrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlXNyMTOzyjm5mJlZ5ZxczMysck4uZmZWOScXMzOrnJOLmZlVzsnFzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6tcrclF0gRJ10l6WNJDkk6QNElSt6RN+T4xy0rSJZJ6JG2UNLt0nK4sv0lSV511NjOzfVf3lctfA38XEW8C3g48BCwD1kVEB7Au1wFOATrytRS4FEDSJGA5cDwwB1jeSEhmZjY21ZZcJL0OeDdwOUBEvBARzwCLgFVZbBVwei4vAq6Iwh3ABElTgJOB7ojYFhHbgW5gQV31NjOzfVfnlctRQB/wTUn3SPqGpIOBIyJiC0C+H57lpwJPlvbvzdhA8d1IWippvaT1fX191bfGzMyaVmdyGQ/MBi6NiGOBX/FyF1h/1E8sBonvHohYERGdEdE5efLkvamvmZlVpM7k0gv0RsSduX4dRbJ5Kru7yPetpfLTS/tPAzYPEjczszGqtuQSEf8EPCnp6AzNAx4E1gCNEV9dwA25vAY4K0eNzQV2ZLfZWmC+pIl5I39+xszMbIwaX/Px/yPwbUn7AY8CZ1MktNWSlgBPAGdm2ZuBhUAP8HyWJSK2Sfo8cHeWOz8ittVcbzMz2we1JpeIuBfo7GfTvH7KBnDOAMdZCaystnZmZlYX/0LfzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlas1uUh6XNJ9ku6VtD5jkyR1S9qU7xMzLkmXSOqRtFHS7NJxurL8JkldddbZzMz23Uhcubw3ImZFRGeuLwPWRUQHsC7XAU4BOvK1FLgUimQELAeOB+YAyxsJyczMxqbR6BZbBKzK5VXA6aX4FVG4A5ggaQpwMtAdEdsiYjvQDSwY6UqbmVnz6k4uAdwiaYOkpRk7IiK2AOT74RmfCjxZ2rc3YwPFdyNpqaT1ktb39fVV3AwzMxuO8TUf/8SI2CzpcKBb0sODlFU/sRgkvnsgYgWwAqCzszOe3pvamplZJWq9comIzfm+Fbie4p7JU9ndRb5vzeK9wPTS7tOAzYPEzcxsjGoquUh663APLOlgSYc2loH5wP3AGqAx4qsLuCGX1wBn5aixucCO7DZbC8yXNDFv5M/PmJmZjVHNdot9TdJ+wLeA70TEM03scwRwvaTG53wnIv5O0t3AaklLgCeAM7P8zcBCoAd4HjgbICK2Sfo8cHeWOz8itjVZbzMzGwVNJZeIeJekDuCjwHpJdwHfjIjuQfZ5FHh7P/FfAPP6iQdwzgDHWgmsbKauZmY2+pq+5xIRm4DPAp8Cfh+4RNLDkv6wrsqZmVlravaey9skXQw8BLwP+IOIeHMuX1xj/czMrAU1e8/lb4CvA5+OiF83gjnM+LO11MzMzFpWs8llIfDriHgRQNJrgAMi4vmIuLK22pmZWUtq9p7LD4ADS+sHZczMzOwVmk0uB0TELxsruXxQPVUyM7NW12xy+dUeU+AfB/x6kPJmZvYq1uw9l08A10pqTLsyBfhQPVUyM7NW1+yPKO+W9CbgaIqJJB+OiN/WWjMzM2tZw5kV+R3AjNznWElExBW11MrMzFpaU8lF0pXAvwLuBV7McABOLmZm9grNXrl0Asfk/F9mZmaDaja53A/8C2BLjXUZc2Ysu6nf+OMXnjrCNTEzay3NJpc3AA/mbMg7G8GIOK2WWpmZWUtrNrl8rs5KmJlZe2l2KPIPJf0O0BERP5B0EDCu3qqZmVmranbK/T8FrgMuy9BU4Pt1VcrMzFpbs9O/nAOcCDwLLz047PC6KmVmZq2t2eSyMyJeaKxIGk/xO5chSRon6R5JN+b6TEl3Stok6RpJ+2V8/1zvye0zSsc4N+OPSDq52caZmdnoaDa5/FDSp4EDJb0fuBb4P03u+3GKJ1g2XARcHBEdwHZgScaXANsj4o0UT7e8CEDSMcBi4C3AAuCrkny/x8xsDGs2uSwD+oD7gD8DbgaGfAKlpGnAqcA3cl0Uj0a+LousAk7P5UW5Tm6fl+UXAVdHxM6IeAzoAeY0WW8zMxsFzY4W+2eKxxx/fZjH/zLwl8Chuf564JmI2JXrvRSDA8j3J/PzdknakeWnAneUjlne5yWSlgJLAY488kg0zIqamVl1mh0t9pikR/d8DbHPB4CtEbGhHO6naAyxbbB9Xg5ErIiIzojonDx58mBVMzOzmg1nbrGGA4AzgUlD7HMicJqkhbnP6yiuZCZIGp9XL9OAxjNieoHpQG8OGDgM2FaKN5T3MTOzMaipK5eI+EXp9fOI+DLFvZPB9jk3IqZFxAyKG/K3RsRHgNuAM7JYF3BDLq/JdXL7rTlR5hpgcY4mmwl0AHc130QzMxtpzU65P7u0+hqKK5lDByg+lE8BV0v6AnAPcHnGLweulNRDccWyGCAiHpC0GngQ2AWcExEvvvKwZmY2VjTbLfY/S8u7gMeBf9Psh0TE7cDtufwo/Yz2iojfUHS39bf/BcAFzX6emZmNrmZHi7237oqYmVn7aLZb7JODbY+IL1VTHTMzawfDGS32Doqb6wB/APw9+bsUMzOzsuE8LGx2RDwHIOlzwLUR8Sd1VczMzFpXs9O/HAm8UFp/AZhReW3MzKwtNHvlciVwl6TrKX4d/0HgitpqZWZmLa3Z0WIXSPpb4PcydHZE3FNftczMrJU12y0GcBDwbET8NcUULTNrqpOZmbW4ZieuXE7xy/pzM/Ra4H/XVSkzM2ttzV65fBA4DfgVQERsZu+nfzEzszbXbHJ5ISeRDABJB9dXJTMza3XNJpfVki6jmC7/T4EfMPwHh5mZ2atEs6PFvijp/cCzwNHAX0VEd601MzOzljVkcpE0DlgbEScBTihmZjakIbvF8tkpz0s6bATqY2ZmbaDZX+j/BrhPUjc5YgwgIj5WS63MzKylNZtcbsqXmZnZkAZNLpKOjIgnImLVSFXIzMxa31D3XL7fWJD03eEcWNIBku6S9BNJD0g6L+MzJd0paZOkayTtl/H9c70nt88oHevcjD8i6eTh1MPMzEbeUMlFpeWjhnnsncD7IuLtwCxggaS5wEXAxRHRAWwHlmT5JcD2iHgjcHGWQ9IxwGLgLcAC4Ks5gs3MzMaooZJLDLA8pCj8Mldfm68A3gdcl/FVwOm5vCjXye3zJCnjV0fEzoh4DOgB5gynLmZmNrKGSi5vl/SspOeAt+Xys5Kek/TsUAeXNE7SvcBWit/I/BR4JiJ2ZZFeYGouTyUfm5zbdwCvL8f72af8WUslrZe0vq+vb6iqmZlZjQa9oR8R+9T9lL+RmSVpAnA98Ob+iuW7Btg2UHzPz1oBrADo7OyMp/eqxs2Zsaz/gXOPX3hqjZ9qZtY6hvM8l70WEc8AtwNzKeYnayS1acDmXO4FpgPk9sOAbeV4P/uYmdkYVFtykTQ5r1iQdCBwEvAQcBtwRhbrAm7I5TW5Tm6/NWdiXgMsztFkM4EO4K666m1mZvuu2R9R7o0pwKoc2fUaYHVE3CjpQeBqSV8A7gEuz/KXA1dK6qG4YlkMEBEPSFoNPAjsAs7J7jYzMxujaksuEbEROLaf+KP0M9orIn4DnDnAsS4ALqi6jmZmVo8RuediZmavLk4uZmZWOScXMzOrnJOLmZlVzsnFzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVrnakouk6ZJuk/SQpAckfTzjkyR1S9qU7xMzLkmXSOqRtFHS7NKxurL8JkldddXZzMyqUeeVyy7gP0XEm4G5wDmSjgGWAesiogNYl+sApwAd+VoKXApFMgKWA8cDc4DljYRkZmZjU23JJSK2RMSPc/k54CFgKrAIWJXFVgGn5/Ii4Ioo3AFMkDQFOBnojohtEbEd6AYW1FVvMzPbd+NH4kMkzQCOBe4EjoiILVAkIEmHZ7GpwJOl3XozNlB8z89YSnHFw5FHHomqbUJTZiy76RWxxy88dRRqYmY2umq/oS/pEOC7wCci4tnBivYTi0HiuwciVkREZ0R0Tp48ee8qa2Zmlag1uUh6LUVi+XZEfC/DT2V3F/m+NeO9wPTS7tOAzYPEzcxsjKpztJiAy4GHIuJLpU1rgMaIry7ghlL8rBw1NhfYkd1na4H5kibmjfz5GTMzszGqznsuJwL/FrhP0r0Z+zRwIbBa0hLgCeDM3HYzsBDoAZ4HzgaIiG2SPg/cneXOj4htNdbbzMz2UW3JJSL+L/3fLwGY10/5AM4Z4FgrgZXV1c7MzOrkX+ibmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlavzYWEGzFh2U7/xxy88dYRrYmY2cnzlYmZmlXNyMTOzytWWXCStlLRV0v2l2CRJ3ZI25fvEjEvSJZJ6JG2UNLu0T1eW3ySpq676mplZdeq8cvkWsGCP2DJgXUR0AOtyHeAUoCNfS4FLoUhGwHLgeGAOsLyRkMzMbOyqLblExN8D2/YILwJW5fIq4PRS/Ioo3AFMkDQFOBnojohtEbEd6OaVCcvMzMaYkb7nckREbAHI98MzPhV4slSuN2MDxc3MbAwbKzf01U8sBom/8gDSUknrJa3v6+urtHJmZjY8I51cnsruLvJ9a8Z7gemlctOAzYPEXyEiVkREZ0R0Tp48ufKKm5lZ80Y6uawBGiO+uoAbSvGzctTYXGBHdputBeZLmpg38udnzMzMxrDafqEv6SrgPcAbJPVSjPq6EFgtaQnwBHBmFr8ZWAj0AM8DZwNExDZJnwfuznLnR8SegwTMzGyMqS25RMSHB9g0r5+yAZwzwHFWAisrrJqZmdVsrNzQNzOzNuKJK0eJJ7Q0s3bmKxczM6uck4uZmVXOycXMzCrn5GJmZpXzDf0xxjf6zawd+MrFzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyHi3WIvobReYRZGY2VvnKxczMKufkYmZmlXNyMTOzyvmeSwvzr/nNbKzylYuZmVXOVy5taKArmoH4SsfMqtYyVy6SFkh6RFKPpGWjXR8zMxtYS1y5SBoHfAV4P9AL3C1pTUQ8OLo1aw/DudLxVY6ZNaMlkgswB+iJiEcBJF0NLAKcXEbYcLvc6jRQovNAB7PR1yrJZSrwZGm9Fzi+XEDSUmBpru5kwwfuH6G6jYY3AE+PdiVq1FT7dNHwDjrc8jXy+Wtd7dw2gKOrOlCrJBf1E4vdViJWACsAJK2PiM6RqNhocPtam9vXutq5bVC0r6pjtcoN/V5geml9GrB5lOpiZmZDaJXkcjfQIWmmpP2AxcCaUa6TmZkNoCW6xSJil6Q/B9YC44CVEfHAILusGJmajRq3r7W5fa2rndsGFbZPETF0KTMzs2FolW4xMzNrIU4uZmZWubZLLq0+TYyk6ZJuk/SQpAckfTzjkyR1S9qU7xMzLkmXZHs3Spo9ui1ojqRxku6RdGOuz5R0Z7bvmhy4gaT9c70nt88YzXo3Q9IESddJejjP4wntdP4k/UX+bd4v6SpJB7Ty+ZO0UtJWSfeXYsM+X5K6svwmSV2j0Zb+DNC+/5F/nxslXS9pQmnbudm+RySdXIoP77s1ItrmRXGz/6fAUcB+wE+AY0a7XsNswxRgdi4fCvwjcAzw34FlGV8GXJTLC4G/pfgt0FzgztFuQ5Pt/CTwHeDGXF8NLM7lrwH/Ppf/A/C1XF4MXDPadW+ibauAP8nl/YAJ7XL+KH7Q/BhwYOm8/XErnz/g3cBs4P5SbFjnC5gEPJrvE3N54mi3bZD2zQfG5/JFpfYdk9+b+wMz8/t03N58t456wyv+j3gCsLa0fi5w7mjXax/bdAPFnGqPAFMyNgV4JJcvAz5cKv9SubH6ovid0jrgfcCN+Q/16dIf+0vnkWKE4Am5PD7LabTbMEjbXpdfvtoj3hbnj5dny5iU5+NG4ORWP3/AjD2+fId1voAPA5eV4ruVG+3Xnu3bY9sHgW/n8m7fmY3ztzffre3WLdbfNDFTR6ku+yy7EI4F7gSOiIgtAPl+eBZrxTZ/GfhL4J9z/fXAMxGxK9fLbXipfbl9R5Yfq44C+oBvZrffNyQdTJucv4j4OfBF4AlgC8X52ED7nL+G4Z6vljqPe/goxdUYVNi+dksuQ04T0yokHQJ8F/hERDw7WNF+YmO2zZI+AGyNiA3lcD9Fo4ltY9F4ii6ISyPiWOBXFN0qA2mp9uW9h0UUXSb/EjgYOKWfoq16/oYyUHtasp2SPgPsAr7dCPVTbK/a127JpS2miZH0WorE8u2I+F6Gn5I0JbdPAbZmvNXafCJwmqTHgaspusa+DEyQ1PhRb7kNL7Uvtx8GbBvJCg9TL9AbEXfm+nUUyaZdzt9JwGMR0RcRvwW+B7yT9jl/DcM9X612HslBBx8APhLZ10WF7Wu35NLy08RIEnA58FBEfKm0aQ3QGIHSRXEvphE/K0exzAV2NC7nx6KIODcipkXEDIrzc2tEfAS4DTgji+3Zvka7z8jyY/b/CCPin4AnJTVml51H8WiItjh/FN1hcyUdlH+rjfa1xfkrGe75WgvMlzQxr+7mZ2xMkrQA+BRwWkQ8X9q0Blico/xmAh3AXezNd+to32iq4cbVQooRVj8FPjPa9dmL+r+L4nJzI3BvvhZS9FOvAzbl+6QsL4oHqf0UuA/oHO02DKOt7+Hl0WJH5R9xD3AtsH/GD8j1ntx+1GjXu4l2zQLW5zn8PsXoobY5f8B5wMPA/cCVFCOLWvb8AVdR3D/6LcX/oS/Zm/NFce+iJ19nj3a7hmhfD8U9lMZ3zNdK5T+T7XsEOKUUH9Z3q6d/MTOzyrVbt5iZmY0BTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJtQdJncqbejZLulXT8aNdpX0j6lqQzhi6518efJWlhaf1zkv5zXZ9nrz4t8Zhjs8FIOoHil8azI2KnpDdQzNxqA5sFdAI3j3ZFrD35ysXawRTg6YjYCRART0fEZgBJx0n6oaQNktaWpvQ4TtJPJP0on21xf8b/WNLfNA4s6UZJ78nl+Vn+x5KuzfnfkPS4pPMyfp+kN2X8EEnfzNhGSX802HGaIem/SLo7j3dexmaoeG7M1/Pq7RZJB+a2d2TZl9qZv7A+H/hQXuV9KA9/jKTbJT0q6WN7fTbMcHKx9nALMF3SP0r6qqTfh5fmaPtfwBkRcRywErgg9/km8LGIOKGZD8iroc8CJ0XEbIpf4H+yVOTpjF8KNLqX/ivF9CC/GxFvA25t4jiD1WE+xXQccyiuPI6T9O7c3AF8JSLeAjwD/FGpnf8u2/kiQES8APwVxbNVZkXENVn2TRTT588Blud/P7O94m4xa3kR8UtJxwG/B7wXuEbFk/LWA28FuotpsBgHbJF0GDAhIn6Yh7iS/mf2LZtL8SClf8hj7Qf8qLS9McHoBuAPc/kkijmYGvXcrmJW6MGOM5j5+bon1w+hSCpPUEwmeW+pDjNUPF3w0Ij4fxn/DkX34UBuyqu/nZK2AkdQTBdiNmxOLtYWIuJF4Hbgdkn3UUw2uAF4YM+rk/zSHWjeo13sfkV/QGM3oDsiPjzAfjvz/UVe/nelfj5nqOMMRsB/i4jLdgsWz/3ZWQq9CBxI/9OkD2bPY/j7wfaau8Ws5Uk6WlJHKTQL+BnFxHuT84Y/kl4r6S0R8QywQ9K7svxHSvs+DsyS9BpJ0ym6iADuAE6U9MY81kGS/vUQVbsF+PNSPSfu5XEa1gIfLd3rmSrp8IEKR8R24LmcvRdKV1HAcxSP0TarhZOLtYNDgFWSHpS0kaLb6XN5b+EM4CJJP6GY/fWduc/ZwFck/Qj4delY/0DxmOL7KJ64+GOAiOijeFb8VfkZd1DcoxjMF4CJeRP9J8B7h3mcyyT15utHEXELRdfWj/Lq7DqGThBLgBXZTlE8CRKKKfKP2eOGvlllPCuyveplt9KNEfHWUa5K5SQdEhG/zOVlFM+F//goV8teBdynatbeTpV0LsW/9Z9RXDWZ1c5XLmZmVjnfczEzs8o5uZiZWeWcXMzMrHJOLmZmVjknFzMzq9z/B9Ol3cUfGS4QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy là 180 là kết quả tương đối hợp lý. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có cảm nhận rõ hơn về dữ liệu, chúng ta có thể hiển thị một số review bất kỳ như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "Chỉ cách phổ cổ khoảng 1km nên khách_sạn này rất tiện_lợi . Do mới xây nên rất mới đẹp .\n",
      "\n",
      "- Phòng_ốc : có loại phòng nhỏ và phòng bự có bồ sục . Mình ở loại phòng nhỏ my mind , ngay trước mặt hồ bơi . Phòng này giá hơn 3 triệu/đêm\n",
      "\n",
      "- Hồ bơi trung_tâm , có hồ bơi nước nóng dưới tầng hầm . Hồi bơi nhỏ thôi nhưng khá đẹp . Hồ bơi dùng nước_biển thật chứ ko phải nước flo nhé .\n",
      "\n",
      "- Massage : khá tốt , mình book được ưu_đãi massage hàng ngày , ngày đầu massage buổi tối không đã lắm nhưng ngày thứ 2 thì rất ok .\n",
      "\n",
      "- Buffet sáng : ngoài các món âu truyền_thống thì ở đây có thêm súhi .\n",
      "\n",
      "- Dịch_vụ phòng tốt . Có xe_đạp miễn_phí cho khách đi_lại phố cổ .\n",
      "\n",
      "Nói_chung là ở đây rất tuyệt !\n",
      "\n",
      "A negative sentence: \n",
      "Khi mình vào thì bàn chưa dọn , dơ kinh . Cái bếp lẩu thì đã bị rỉ sét , phục_vụ thì mặt kg 1 nụ_cười ( mặt như đưa_đám ) , phải kêu đi kêu lại mấy lần mới đem tới . Khi dem tới thì \" quăng \" ngay bàn mà kg nói 1 tiếng nào . Chẳng còn gì để nói về quán này . Cực_kì tệ\n",
      "\n",
      "Mình chẳng bao_giờ comment , nhưng bức_xúc với nơi đây quá nên phải \" gõ \" để cảnh_báo . Tốn tiền mà mang bệnh tức .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[3] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[10] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn hoá văn bản và tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tiết kiệm công sức và cũng nằm ngoài phạm vi của khoá học, chúng tôi đã chuẩn bị sẵn tập dữ liệu đã được tách từ. Giữa hai từ có thể ghép lại để tạo thành một khái niệm mới chúng tôi sử dụng ký tự '_' để nối các từ đó. Ví dụ: 'sinh_viên', 'sinh_học'.\n",
    "\n",
    "Chúng tôi chuẩn bị sẵn các hàm chuẩn hoá văn bản nhằm loại bỏ các ký tự đặc biệt. Tham khảo ở hàm 'cleanSentences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile('[^\\w0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ biểu diễn 30.000 review dưới dạng các chỉ số của các từ. Tập dữ liệu positive và negative sẽ được tính hợp lại thành một ma trận 30000x180. Trong đó 30000 là số lượng review và 180 là số lượng từ tối đa cho một câu. Do bước chuẩn bị này tốn khá nhiều tài nguyên tính toán nên sau khi tính toán xong, chúng ta sẽ lưu lại để sử dụng cho những lần chạy thí nghiệm sau. Ma trận lưu trữ các chỉ số này là: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: xác định chỉ số của từng từ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này chúng ta sẽ tiến hành tra cứu từng từ trong review, sau đó gán vào ma trận 'ids'. Trong đó chỉ số dòng của ma trận tương ứng với file review, chỉ số cột của ma trận tương ứng với một từ của review. Trường hợp từ nào không có trong tập từ điển thì ta sẽ gán bằng chỉ số của từ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype=np.int32)\n",
    "nFiles = 0\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf_idx in range(len(positiveFiles)):\n",
    "    with open(positiveFiles[pf_idx], 'r', encoding='utf-8') as f:\n",
    "        # nIndexes = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word_idx in range(len(split)):\n",
    "            if word_idx >= maxSeqLength:\n",
    "                break\n",
    "            try:\n",
    "                idx = wordsList.index(split[word_idx])\n",
    "                ids[pf_idx, word_idx] = idx\n",
    "            except ValueError:\n",
    "                ids[pf_idx, word_idx] = unk_idx\n",
    "\n",
    "for nf_idx in range(len(positiveFiles), len(positiveFiles)+len(negativeFiles)):\n",
    "    with open(negativeFiles[nf_idx-len(positiveFiles)], 'r', encoding='utf-8') as f:\n",
    "        # nIndexes = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word_idx in range(len(split)):\n",
    "            if word_idx >= maxSeqLength:\n",
    "                break\n",
    "            try:\n",
    "                idx = wordsList.index(split[word_idx])\n",
    "                ids[nf_idx, word_idx] = idx\n",
    "            except ValueError:\n",
    "                ids[nf_idx, word_idx] = unk_idx\n",
    "            \n",
    "np.save('idsMatrix.npy', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821\n",
      " 14794  8884  6443  5767  8589 18850 15570  5596   799 11060  4222 16893\n",
      " 13078  8136  3364  4454  4756 10304  8885  3553  9782  1232 14359 10606\n",
      "   579 15522  2219 15092 14855 15253  4884  3364  5519  4558  9649   269\n",
      " 15522 12309 14855 11503  2212  4884  7155 11577  4222  5767 15076 12225\n",
      " 10774  1218  2876 19584  4558  2974 13452  5013   842 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  4135  3553 14967  4964\n",
      " 15385  9673  2997 14855  7446  8038 11440  1345   842  5767   803 11060\n",
      " 18791  5013     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# LƯU Ý: Bước thực hiện trên tương đối mất thời gian.\n",
    "# Trường hợp đã tính toán và lưu ma trận 'ids' rồi thì ta có thể load lên để sử dụng luôn\n",
    "\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "#ids = np.load(f'{currentDir}idsMatrix.npy')\n",
    "print('Word indexes of the first review: ', ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như quá trình chuyển từ câu dạng văn bảng sang vector các chỉ số trong từ điển ở trên đúng thì ids[0] sẽ nhận giá trị: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng hàm lấy dữ liệu train và test theo từng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây chúng tôi xây dựng các hàm để lấy dữ liệu train và test theo từng batch. Bạn hãy giải thích tại sao lại có các con số 13999, 14999, 15999, 29999 nhé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros((batchSize, maxSeqLength))\n",
    "    for i in range(batchSize):\n",
    "        if i % 2 == 0:\n",
    "            # Pick positive samples randomly\n",
    "            num = randint(1, 14000)\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            # Pick negative samples randomly\n",
    "            num = randint(16001, 30000)\n",
    "            labels.append([0, 1])\n",
    "            \n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros((batchSize, maxSeqLength))\n",
    "    for i in range(batchSize):\n",
    "        num = randint(14001, 16000)\n",
    "        if num <= 15000:\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            labels.append([0, 1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Xây dựng RNN Model với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên chúng tôi sẽ khởi tạo các tham số cho mô hình mạng RNN với các cell là các LSTM. Kiến trúc mạng ở đây bao gồm 128 đơn vị cho mỗi lớp, số lượng layer là 2, số lượng phân lớp là 2 và số vòng lặp khi huấn luyện là 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lưu trữ dữ liệu input và ouput, chúng ta sẽ sử dụng hai kiểu dữ liệu placeholder. Một trong những điều quan trọng nhất khi khởi tạo các biến input và output này là xác định kích thước của các tensor. Mỗi output của mạng (hay còn gọi là label) sẽ là một vector one hot với hai giá trị tương ứng với hai loại cảm xúc: [1, 0] cho positive và [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: Xác định input và output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo hai biến 'inputs' và 'labels' bằng kiểu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO 3.3: Khởi tạo hai biến 'inputs' và 'labels'\n",
    "labels = tf.placeholder(tf.float32, shape=[batchSize, numClasses], name='labels')\n",
    "inputs = tf.placeholder(tf.int32, shape=[batchSize, maxSeqLength], name='inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó tạo dữ liệu word vector từ khối dữ liệu đầu vào với ma trận word embedding. Nếu như quá trình khởi tạo đúng thì sẽ tạo ra các kiểu dữ liệu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 10), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros((batchSize, maxSeqLength, numDimensions)), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy sau bước này chúng ta đã có dữ liệu để đưa vào mạng mạng các LSTM. Để khởi tạo một LSTM chúng ta sử dụng hàm tf.nn.rnn_cell.BasicLSTMCell. Hàm này cần tham số đầu vào là số lượng đơn vị muốn khởi tạo. Đây chính là một hyperparamter đã được khởi tạo trước đó.\n",
    "Để chống lại việc overfitting, chúng ta sử dụng lớp dropout. \n",
    "\n",
    "Để tăng tính phức tạp cho kiến trúc mạng chúng ta chồng các lớp LSTM lên nhau (Stack LSTM Layers). Trong trường hợp này chúng ta sử dụng 2 lớp LSTM. Việc chồng thêm các lớp LSTM sẽ giúp cho mô hình có khả năng nhớ nhiều thông tin hơn nhưng đồng thời cũng làm tăng số lượng tham số khi huấn luyện. Điều này cũng có nghĩa là sẽ làm tăng thời gian huấn luyện cũng như là cần thêm nhiều dữ liệu hơn.\n",
    "\n",
    "Cuối cùng là đưa toàn bộ dữ liệu đầu vào vào mạng các LSTM sử dụng hàm tf.nn.dynamic_rnn. Chi tiết kiến trúc mạng LSTM sử dụng cho bài tập này được mô tả trong hình sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-e64423e3ead4>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Tensor(\"rnn/transpose_1:0\", shape=(64, 180, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def generate_a_lstm_layer():\n",
    "    # Khởi tạo một LSTM layer với 'lstmUnits' unit sử dụng hàm tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "    # Sau đó tạo một lớp dropout để chống overfitting với hệ số out_keep_prob bằng 0.75\n",
    "    # Sử dụng hàm tf.contrib.rnn.DropoutWrapper\n",
    "    basic_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=lstmUnits)\n",
    "    drop_out = tf.contrib.rnn.DropoutWrapper(cell=basic_lstm, output_keep_prob=0.75)\n",
    "    return drop_out\n",
    "\n",
    "# Sau khi đã có hàm tạo một LSTM Layer, ta sử dụng hàm này để chồng các LSTM lên\n",
    "# Stack các LSTM layer với hàm tf.nn.rnn_cell.MultiRNNCell\n",
    "multi_layer = tf.nn.rnn_cell.MultiRNNCell([generate_a_lstm_layer() for _ in range(nLayers)])\n",
    "# Feed data variable vào mạng LSTM sử dụng hàm tf.nn.dynamic_rnn\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer, data, dtype=tf.float32)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra khỏi mạng LSTM, biến outputs sẽ là một tensor có kích thước [batchSize x maxSeqLength x lstmUnits], cụ thể là [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, chúng ta chỉ lấy dữ liệu ở LSTM cell cuối cùng và cho đi qua lớp kết nối đầy đủ để phân loại thành 2 trạng thái. Chỉ số của LSTM cell cuối cùng là 179 (do có 180 cell theo chiều ngang)  nên để có thể lấy được giá trị ta sẽ chuyển vị về tensor có kích thước [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. Sử dụng hàm tf.gather để lấy tensor thứ 179 có kích thước [64 x 128] bao gồm 64 mẫu vector 128 chiều. Vector 128 chiều này sẽ được đưa vào lớp fully connected để chuyển đổi về vector 2 chiều tương ứng với 2 trạng thái.\n",
    "\n",
    "Lớp kết nối đầy đủ bao gồm các bộ tham số 'weight' và 'bias' để thực hiện việc dự đoán kết quả. Bước này chính là tạo một lớp Fully Connected như trong sơ đồ kiến trúc mạng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "# Lấy giá trị output tại LSTM cell cuối cùng\n",
    "outputs = tf.transpose(outputs, [1, 0 , 2])\n",
    "last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "\n",
    "# Đưa qua mạng Fully Connected mà không có activation function\n",
    "predict = tf.matmul(last, weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định độ chính xác của hệ thống, ta đếm số lượng labels khớp với giá trị dự đoán (prediction). Sau đó tính độ chính xác bằng cách tính giá trị trung bình của các kết quả trả về đúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_result = tf.equal(tf.argmax(predict, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_result, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó chúng ta sẽ xác định hàm độ lỗi sử dụng softmax cross entropy được tính từ dữ liệu dự đoán và tập labels. Cuối cùng là chọn thuật toán tối ưu với tham số learning rate mặc định là 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-4be5307e4de4>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Tensorboard để visualize kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong quá trình huấn luyện, chương trình sẽ ghi log về độ lỗi và độ chính xác trên tập train vào thư mục 'tensorboard', lưu lại model sau mỗi 2000 vòng lặp ở thư mục 'models'. Việc huấn luyện trên 30,000 vòng lặp mất khoảng vài tiếng với GPU K80 được cung cấp bởi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mỗi vòng lặp, ta sẽ lấy ra một batch dữ liệu train để đưa vào mạng sử dụng `feed_dict`. với các tham số input và label là các placeholders. Bước huấn luyện này được lặp lại cho đến khi hết số lần cần huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-2000\n",
      "saved to models/pretrained_lstm.ckpt-4000\n",
      "saved to models/pretrained_lstm.ckpt-6000\n",
      "saved to models/pretrained_lstm.ckpt-8000\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "saved to models/pretrained_lstm.ckpt-12000\n",
      "saved to models/pretrained_lstm.ckpt-14000\n",
      "saved to models/pretrained_lstm.ckpt-16000\n",
      "saved to models/pretrained_lstm.ckpt-18000\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "saved to models/pretrained_lstm.ckpt-22000\n",
      "saved to models/pretrained_lstm.ckpt-24000\n",
      "saved to models/pretrained_lstm.ckpt-26000\n",
      "saved to models/pretrained_lstm.ckpt-28000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TODO 3.5\n",
    "    # Get next training batch\n",
    "    next_batch, next_batch_labels = getTrainBatch()\n",
    "    # Feed to optimizer\n",
    "    sess.run(optimizer, feed_dict={inputs: next_batch, labels: next_batch_labels})\n",
    "    \n",
    "    # Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {inputs: next_batch, labels: next_batch_labels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    # Save model every 2000 training iterations\n",
    "    if (i % 2000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load mô hình đã train và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian huấn luyện mạng khá lâu, nên trong quá trình mạng đang được huấn luyện, ta sẽ lưu lại một số checkpoint. Để có thể test thử trên một checkpoint mới nhất ta sử dụng hàm tf.train.latest_checkpoint và truyền vào tên thư mục muốn lấy model mới nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models_3\\pretrained_lstm.ckpt-30000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models_3')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, với mỗi batch dữ liệu test, ta sẽ tiến hành test và tính độ chính xác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do các bộ test được lấy ngẫu nhiên nên độ chính xác trong quá trình này cũng dao động từ 70% đến 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 71.875\n",
      "Accuracy for this batch: 71.875\n",
      "Accuracy for this batch: 79.6875\n",
      "Accuracy for this batch: 78.125\n",
      "Accuracy for this batch: 81.25\n",
      "Accuracy for this batch: 79.6875\n",
      "Accuracy for this batch: 71.875\n",
      "Accuracy for this batch: 73.4375\n",
      "Accuracy for this batch: 84.375\n",
      "Accuracy for this batch: 75.0\n"
     ]
    }
   ],
   "source": [
    "# Test on 10 batches\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    next_batch, next_batch_labels = getTestBatch()\n",
    "    # TODO 3.6: Tính độ chính xác 'accuracy' trên các test batch và gán vào 'test_acc'\n",
    "    test_acc = sess.run(accuracy, {inputs: next_batch, labels: next_batch_labels}) * 100\n",
    "    print(\"Accuracy for this batch:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Viết hàm tổng hợp để dự đoán cảm xúc từ câu tiếng Việt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu cuối cùng này đòi hỏi đòi hỏi các bạn phải vận dụng tư duy tổng hợp để gom tất cả những bước đã thực hiện trước đó thành một quy trình hoàn chỉnh. Các bạn cần viết một hàm hoàn chỉnh với đầu vào là  một câu tiếng Việt cho trước, đầu ra là cho biết câu trên có cảm xúc tích cực hay tiêu cực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_batch_size(input_sentence):\n",
    "    arr = np.zeros((batchSize, maxSeqLength), dtype=np.int32)\n",
    "    cleanedLine = cleanSentences(input_sentence)\n",
    "    split = cleanedLine.split()\n",
    "    unk_idx = wordsList.index('UNK')\n",
    "    \n",
    "    tmp = np.zeros(maxSeqLength)\n",
    "    for word_idx in range(len(split)):\n",
    "        if word_idx >= maxSeqLength:\n",
    "            break\n",
    "        try:\n",
    "            idx = wordsList.index(split[word_idx])\n",
    "            tmp[word_idx] = idx\n",
    "        except ValueError:\n",
    "            tmp[word_idx] = unk_idx\n",
    "    \n",
    "    arr[0] = tmp\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment is Positive\n",
      "The probability of Positive comment is 0.9983\n",
      "The probability of Nehative comment is 0.0017\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "# from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "input_sentence = 'Phải nói thái độ phục vụ và dịch vụ ở đây cực tốt, phòng đẻ sạch sẽ sang trọng'\n",
    "input_sentence = word_tokenize(input_sentence, format='text')\n",
    "input_sentence = input_to_batch_size(input_sentence)\n",
    "\n",
    "# TODO 3.7 Các bạn vận dụng toàn bộ quy trình đã thực hiện trước đó\n",
    "# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\n",
    "# Câu này làm khá dài và có tính chất tổng hợp\n",
    "predict_sentence = sess.run(predict, feed_dict={inputs: input_sentence})\n",
    "idx = sess.run(tf.argmax(predict_sentence,1))\n",
    "if idx[0] == 1:\n",
    "    print('The comment is Negative')\n",
    "else:\n",
    "    print('The comment is Positive')\n",
    "    \n",
    "proba = sess.run(tf.nn.softmax(predict_sentence[0]))\n",
    "print('The probability of Positive comment is %.4f' % proba[0])\n",
    "print('The probability of Nehative comment is %.4f' % proba[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy qua bài tập này, các bạn được ôn lại mô hình Word2Vec và sử dụng mô hình này để biểu diễn cho một văn bản. Sử dụng cách biểu diễn này để đưa vào mô hình RNN với nhiều đơn vị LSTM. Các bạn có thể thử nghiệm trên các cấu hình khác nhau bằng cách thay đổi các hyperparameter."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
